{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b6fcb16",
   "metadata": {},
   "source": [
    "# Appendix - Knowledge Graph and Deep Learning Python Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0184e0a",
   "metadata": {},
   "source": [
    "## 1) Knowledge Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c35b7",
   "metadata": {},
   "source": [
    "Let's leverage the reviews of the movie \"Sisters\" in our corpus to generate knowledge graphs that can visualize relationships between named entities in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6459c",
   "metadata": {},
   "source": [
    "### 1.1) Loading the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0a8272",
   "metadata": {},
   "source": [
    "First, let's load the corpus into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d195f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U pip setuptools wheel\n",
    "# !pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b00d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " conda install -c conda-forge g2p-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4736ff57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'g2p_en'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mg2p_en\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m G2p\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'g2p_en'"
     ]
    }
   ],
   "source": [
    "# from g2p_en import G2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "412e1031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Using cached tensorflow_datasets-4.9.2-py3-none-any.whl (5.4 MB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\steve\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.4.0)\n",
      "Collecting array-record (from tensorflow_datasets)\n",
      "  Obtaining dependency information for array-record from https://files.pythonhosted.org/packages/4b/93/2a07d2f7317ea17ea9e17865d27175db13292ca8496aaeb6b9614aabe638/array_record-0.4.0-py39-none-any.whl.metadata\n",
      "  Downloading array_record-0.4.0-py39-none-any.whl.metadata (502 bytes)\n",
      "Requirement already satisfied: click in c:\\users\\steve\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (8.0.4)\n",
      "Collecting dm-tree (from tensorflow_datasets)\n",
      "  Using cached dm_tree-0.1.8-cp39-cp39-win_amd64.whl (101 kB)\n",
      "Collecting etils[enp,epath]>=0.9.0 (from tensorflow_datasets)\n",
      "  Obtaining dependency information for etils[enp,epath]>=0.9.0 from https://files.pythonhosted.org/packages/4a/6a/d58ec120f5e4babbf5001c144266ba623dcdae8e81dc6cdb422a98d0e0ce/etils-1.4.1-py3-none-any.whl.metadata\n",
      "  Downloading etils-1.4.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\steve\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.25.1)\n",
      "Collecting promise (from tensorflow_datasets)\n",
      "  Using cached promise-2.3-py3-none-any.whl\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (4.21.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\steve\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (5.8.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (2.27.1)\n",
      "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
      "  Using cached tensorflow_metadata-1.13.1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: termcolor in c:\\users\\steve\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (2.3.0)\n",
      "Requirement already satisfied: toml in c:\\users\\steve\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\steve\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (4.64.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\steve\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.12.1)\n",
      "Collecting importlib_resources (from etils[enp,epath]>=0.9.0->tensorflow_datasets)\n",
      "  Obtaining dependency information for importlib_resources from https://files.pythonhosted.org/packages/29/d1/bed03eca30aa05aaf6e0873de091f9385c48705c4a607c2dfe3edbe543e8/importlib_resources-6.0.0-py3-none-any.whl.metadata\n",
      "  Downloading importlib_resources-6.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\steve\\anaconda3\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (4.7.1)\n",
      "Requirement already satisfied: zipp in c:\\users\\steve\\anaconda3\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (3.7.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\steve\\anaconda3\\lib\\site-packages (from click->tensorflow_datasets) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\steve\\anaconda3\\lib\\site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from tensorflow-metadata->tensorflow_datasets) (1.53.0)\n",
      "Downloading array_record-0.4.0-py39-none-any.whl (3.0 MB)\n",
      "   ---------------------------------------- 3.0/3.0 MB 13.7 MB/s eta 0:00:00\n",
      "Downloading etils-1.4.1-py3-none-any.whl (135 kB)\n",
      "   ---------------------------------------- 135.8/135.8 kB ? eta 0:00:00\n",
      "Downloading importlib_resources-6.0.0-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: dm-tree, promise, importlib_resources, etils, tensorflow-metadata, array-record, tensorflow_datasets\n",
      "Successfully installed array-record-0.4.0 dm-tree-0.1.8 etils-1.4.1 importlib_resources-6.0.0 promise-2.3 tensorflow-metadata-1.13.1 tensorflow_datasets-4.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\steve\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\steve\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a7c0db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "tf.random.set_seed(2022)\n",
    "import tensorflow.keras.backend as k\n",
    "\n",
    "from typing import List, Callable, Dict, Tuple, Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7389f4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\steve\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\steve\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "# Requires Restart of Runtime after Installation\n",
    "\n",
    "!python -m spacy download en_core_web_lg -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9b9d828",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ba8a0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only run this once, they will be downloaded.\n",
    "nltk.download('stopwords',quiet=True)\n",
    "nltk.download('wordnet',quiet=True)\n",
    "nltk.download('punkt',quiet=True)\n",
    "nltk.download('omw-1.4',quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f2a5f2",
   "metadata": {},
   "source": [
    "\n",
    "Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Target audience is the natural language processing (NLP) and information retrieval (IR) community: https://pypi.org/project/gensim/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1263d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "#pkg_resources.require(\"gensim<=3.8.3\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02c9d8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genism Version:  4.1.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Genism Version: \", gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b88a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Warning Messages\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4598d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_movie_descriptor(data: pd.DataFrame, corpus_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Adds \"Movie Description\" to the supplied dataframe, in the form {Genre}_{P|N}_{Movie Title}_{DocID}\n",
    "    \"\"\"\n",
    "    review = np.where(corpus_df['Review Type (pos or neg)'] == 'Positive', 'P', 'N')\n",
    "    data['Descriptor'] = corpus_df['Genre of Movie'] + '_' + corpus_df['Movie Title'] + '_' + review + '_' + corpus_df['Doc_ID'].astype(str)\n",
    "\n",
    "def get_corpus_df(path: str) -> pd.DataFrame:\n",
    "    data = pd.read_csv(path,encoding=\"utf-8\")\n",
    "    add_movie_descriptor(data, data)\n",
    "    sorted_data = data.sort_values(['Descriptor'])\n",
    "    indexed_data = sorted_data.set_index(['Doc_ID'])\n",
    "    indexed_data['Doc_ID'] = indexed_data.index\n",
    "    return indexed_data\n",
    "\n",
    "corpus_df = get_corpus_df('MSDS453_ClassCorpus_Final_Sec56_v5_20230720.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3109c",
   "metadata": {},
   "source": [
    "### 1.2) Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cbcd72",
   "metadata": {},
   "source": [
    "Having loaded the appropriate Python libraries and loaded the corpus of movie reviews, let's conduct some initial exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c469d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Doc_ID</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DSI_Title</th>\n",
       "      <td>SAR_Doc1_Covenant</td>\n",
       "      <td>SAR_Doc2_Covenant</td>\n",
       "      <td>SAR_Doc3_Covenant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Submission File Name</th>\n",
       "      <td>SAR_Doc1_Covenant</td>\n",
       "      <td>SAR_Doc2_Covenant</td>\n",
       "      <td>SAR_Doc3_Covenant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Student Name</th>\n",
       "      <td>SAR</td>\n",
       "      <td>SAR</td>\n",
       "      <td>SAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Genre of Movie</th>\n",
       "      <td>Action</td>\n",
       "      <td>Action</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Review Type (pos or neg)</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Movie Title</th>\n",
       "      <td>Covenant</td>\n",
       "      <td>Covenant</td>\n",
       "      <td>Covenant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text</th>\n",
       "      <td>Nearly two years after the American military w...</td>\n",
       "      <td>Have Guy Ritchie and Jake Gyllenhaal switched ...</td>\n",
       "      <td>Guy Ritchie's The Covenant notably marks the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Descriptor</th>\n",
       "      <td>Action_Covenant_N_101</td>\n",
       "      <td>Action_Covenant_N_102</td>\n",
       "      <td>Action_Covenant_N_103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_ID</th>\n",
       "      <td>101</td>\n",
       "      <td>102</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raw_sentences</th>\n",
       "      <td>[Nearly two years after the American military ...</td>\n",
       "      <td>[Have Guy Ritchie and Jake Gyllenhaal switched...</td>\n",
       "      <td>[Guy Ritchie's The Covenant notably marks the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Doc_ID                                                                  101  \\\n",
       "DSI_Title                                                 SAR_Doc1_Covenant   \n",
       "Submission File Name                                      SAR_Doc1_Covenant   \n",
       "Student Name                                                            SAR   \n",
       "Genre of Movie                                                       Action   \n",
       "Review Type (pos or neg)                                           Negative   \n",
       "Movie Title                                                        Covenant   \n",
       "Text                      Nearly two years after the American military w...   \n",
       "Descriptor                                            Action_Covenant_N_101   \n",
       "Doc_ID                                                                  101   \n",
       "raw_sentences             [Nearly two years after the American military ...   \n",
       "\n",
       "Doc_ID                                                                  102  \\\n",
       "DSI_Title                                                 SAR_Doc2_Covenant   \n",
       "Submission File Name                                      SAR_Doc2_Covenant   \n",
       "Student Name                                                            SAR   \n",
       "Genre of Movie                                                       Action   \n",
       "Review Type (pos or neg)                                           Negative   \n",
       "Movie Title                                                        Covenant   \n",
       "Text                      Have Guy Ritchie and Jake Gyllenhaal switched ...   \n",
       "Descriptor                                            Action_Covenant_N_102   \n",
       "Doc_ID                                                                  102   \n",
       "raw_sentences             [Have Guy Ritchie and Jake Gyllenhaal switched...   \n",
       "\n",
       "Doc_ID                                                                  103  \n",
       "DSI_Title                                                 SAR_Doc3_Covenant  \n",
       "Submission File Name                                      SAR_Doc3_Covenant  \n",
       "Student Name                                                            SAR  \n",
       "Genre of Movie                                                       Action  \n",
       "Review Type (pos or neg)                                           Negative  \n",
       "Movie Title                                                        Covenant  \n",
       "Text                      Guy Ritchie's The Covenant notably marks the f...  \n",
       "Descriptor                                            Action_Covenant_N_103  \n",
       "Doc_ID                                                                  103  \n",
       "raw_sentences             [Guy Ritchie's The Covenant notably marks the ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize sentences\n",
    "def get_sentences(text: str) -> List[str]:\n",
    "    return [str(x) for x in nlp(text).sents]\n",
    "\n",
    "corpus_df['raw_sentences'] = corpus_df.Text.apply(get_sentences)\n",
    "corpus_df.head(3).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6442573e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f10c1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 190 entries, 101 to 217\n",
      "Data columns (total 10 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   DSI_Title                 190 non-null    object\n",
      " 1   Submission File Name      190 non-null    object\n",
      " 2   Student Name              190 non-null    object\n",
      " 3   Genre of Movie            190 non-null    object\n",
      " 4   Review Type (pos or neg)  190 non-null    object\n",
      " 5   Movie Title               190 non-null    object\n",
      " 6   Text                      190 non-null    object\n",
      " 7   Descriptor                190 non-null    object\n",
      " 8   Doc_ID                    190 non-null    int64 \n",
      " 9   raw_sentences             190 non-null    object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 16.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(corpus_df.info());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e7e8b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Covenant' 'Inception' 'No time to die' 'Taken' 'The Dark Knight Rises'\n",
      " 'Despicable Me 3' 'Holmes and Watson' 'Legally Blonde' 'Lost City'\n",
      " 'Sisters' 'Drag Me to Hell' 'Fresh' 'It Chapter Two' 'The Toxic Avenger'\n",
      " 'US' 'Annihilation' 'Minority Report' 'Oblivion' 'Pitch Black']\n"
     ]
    }
   ],
   "source": [
    "print(corpus_df['Movie Title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a80c9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DSI_Title', 'Submission File Name', 'Student Name', 'Genre of Movie',\n",
       "       'Review Type (pos or neg)', 'Movie Title', 'Text', 'Descriptor',\n",
       "       'Doc_ID', 'raw_sentences'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fbb0746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre of Movie</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Action</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comedy</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Horror</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Genre of Movie  Count\n",
       "0         Action     50\n",
       "1         Comedy     50\n",
       "2         Horror     50\n",
       "3         Sci-Fi     40"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gather the number of reviews by genre\n",
    "counts_df = corpus_df[['Genre of Movie']].copy()\n",
    "counts_df['Count'] = 1\n",
    "counts_df.groupby(['Genre of Movie']).count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e0b5a4",
   "metadata": {},
   "source": [
    "### 1.3) Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c55b08d",
   "metadata": {},
   "source": [
    "Let's conduct data wrangling to prepare the text as needed prior to the creation of our knowledge graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba378177",
   "metadata": {},
   "source": [
    "#### 1.3.1) Data Wrangling and Vectorization Method 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6566c1b8",
   "metadata": {},
   "source": [
    "Let's define data wrangling utility functions that we can leverage on our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0115afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text: str) -> str:\n",
    "    return re.sub('[^a-zA-Z]', '', str(text))\n",
    "\n",
    "def remove_tags(text: str) -> str:    \n",
    "    return re.sub('&lt;/?.*?&gt;', '', text)\n",
    "\n",
    "def remove_special_chars_and_digits(text: str) -> str:\n",
    "    return re.sub('(\\\\d|\\\\W)+', '', text)\n",
    "\n",
    "def get_coref_resolved_sentences(text: str) -> List[str]:\n",
    "    return [str(x) for x in nlp(text).sents]\n",
    "\n",
    "def get_lemmas(text: str, stopwords: Set[str]) -> List[str]:\n",
    "    initial = [remove_tags(remove_special_chars_and_digits(remove_punctuation(x.lemma_.lower()))) for x in nlp(text)]\n",
    "    return [x for x in initial if x not in stopwords]\n",
    "\n",
    "def lemmatize_sentence(text: str, stopwords: Set[str]) -> str:\n",
    "    return ' '.join(get_lemmas(text, stopwords))\n",
    "\n",
    "def clean_doc(doc): \n",
    "    #doc = remove_punctuation(doc)\n",
    "    doc= ' '.join(remove_stop_words(doc))\n",
    "    doc = apply_lemmatization(doc)\n",
    "    return doc\n",
    "\n",
    "def remove_stop_words(in_text):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(in_text)  \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    return filtered_sentence\n",
    "\n",
    "def apply_lemmatization(in_text):\n",
    "    # Lemmatization\n",
    "    lem = WordNetLemmatizer()\n",
    "    word_list = nltk.word_tokenize(in_text)\n",
    "    output = ' '.join([lem.lemmatize(w) for w in word_list])\n",
    "    return output\n",
    "\n",
    "def counter_word(text):\n",
    "  count=Counter()\n",
    "  for i in text.values:\n",
    "    for word in i.split():\n",
    "      count[word]=+1\n",
    "  return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e14a6f8",
   "metadata": {},
   "source": [
    "Tokenize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ac655",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_stopwords=\\\n",
    "set(nltk.corpus.stopwords.words('english')).union(set(nlp.Defaults.stop_words)).union({' ', ''})\n",
    "corpus_df['lemmas'] = corpus_df.Text.apply(lambda x: get_lemmas(x, default_stopwords))\n",
    "corpus_df.lemmas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b0feb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df.head(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0361065b",
   "metadata": {},
   "source": [
    "Reassemble Lemmatized Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dae298",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df['lemmas_joined'] = corpus_df.lemmas.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85daed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df['lemmas_joined'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34ef44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df.head(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8867fca5",
   "metadata": {},
   "source": [
    "Vocabulary Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67215868",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "transformed_documents = vectorizer.fit_transform(corpus_df.lemmas_joined)\n",
    "doc_term_matrix = transformed_documents.todense()\n",
    "doc_term_df = pd.DataFrame(doc_term_matrix, \n",
    "                           columns=vectorizer.get_feature_names_out(), \n",
    "                           index=corpus_df.Descriptor)\n",
    "print(f'All Word Vocabulary size: {doc_term_df.shape[1]}')\n",
    "all_words = set(doc_term_df.columns)\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), min_df=5, max_df=.8)\n",
    "transformed_documents = vectorizer.fit_transform(corpus_df.lemmas_joined)\n",
    "doc_term_matrix = transformed_documents.todense()\n",
    "doc_term_df = pd.DataFrame(doc_term_matrix, \n",
    "                           columns=vectorizer.get_feature_names_out(), \n",
    "                           index=corpus_df.Descriptor)\n",
    "print(f'Curated Vocabulary size: {doc_term_df.shape[1]}')\n",
    "vocabulary = set(doc_term_df.columns)\n",
    "\n",
    "words_to_remove = default_stopwords.union(all_words - vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b9e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "plt.hist(doc_term_df.sum(axis=0).T, range=(0, 200))\n",
    "plt.xlabel('Number of Occurrences',fontsize=14)\n",
    "plt.ylabel('Number of Terms',fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaa26a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-tokenize words, recreate joined documents\n",
    "corpus_df['lemmas'] = corpus_df.Text.apply(lambda x: get_lemmas(x, words_to_remove))\n",
    "corpus_df['lemmas_joined'] = corpus_df.lemmas.apply(lambda x: ' '.join(x))\n",
    "corpus_df.lemmas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a914ff",
   "metadata": {},
   "source": [
    "Get Lemmatized and Filtered Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a71cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df['sentences_lemmatized']=\\\n",
    "corpus_df.raw_sentences.apply(lambda x: [lemmatize_sentence(s, words_to_remove) for s in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00687f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1373d1be",
   "metadata": {},
   "source": [
    "Review Documents By Movie Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa9e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus_df['Movie Title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f5d960",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = corpus_df[corpus_df['Movie Title'] == 'Dirty Grandpa'].copy()\n",
    "movie_df.head(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa771df",
   "metadata": {},
   "source": [
    "#### 1.3.2) Data Wrangling and Vectorization Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae50976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b29aef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb4ba6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4502d688",
   "metadata": {},
   "source": [
    "#### 1.3.3) Data Wrangling and Vectorization Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2833e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d91ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c9371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0d9eb7c",
   "metadata": {},
   "source": [
    "### 1.4) Knowledge Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8926fe90",
   "metadata": {},
   "source": [
    "Let's define knowledge context graph functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aff36da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_edges(map_to: str, map_from: Set[str], df: pd.DataFrame):\n",
    "    print(f'Before mapping {\", \".join(map_from)} -> {map_to}: {sum(df.edge == map_to)}')\n",
    "    df['edge'] = np.where(kg_df.edge.isin(map_from), map_to, kg_df.edge)\n",
    "    print(f'After mapping {\", \".join(map_from)} -> {map_to}: {sum(df.edge == map_to)}')\n",
    "    \n",
    "def map_sources_and_targets(map_to: str, map_from: Set[str], df: pd.DataFrame):\n",
    "    before = sum(df.source == map_to) + sum(df.target == map_to)\n",
    "    print(f'Before mapping {\", \".join(map_from)} -> {map_to}: {before}')\n",
    "    \n",
    "    df['source'] = np.where(kg_df.source.isin(map_from), map_to, kg_df.source)\n",
    "    df['target'] = np.where(kg_df.target.isin(map_from), map_to, kg_df.target)\n",
    "    \n",
    "    after = sum(df.source == map_to) + sum(df.target == map_to)\n",
    "    print(f'After mapping {\", \".join(map_from)} -> {map_to}: {after}')\n",
    "    \n",
    "def get_neighborhood(sources: Set[str], edge_types: Set[str], depth: int, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    output = []\n",
    "    \n",
    "    for d in range(depth):\n",
    "        if edge_types is not None:\n",
    "            rows = df[(df.edge.isin(edge_types)) & ((df.source.isin(sources)) | (df.target.isin(sources)))].copy()\n",
    "        else:\n",
    "            rows = df[(df.source.isin(sources)) | (df.target.isin(sources))].copy()\n",
    "            \n",
    "        output.append(rows)\n",
    "        sources = set(rows.target).union(set(rows.source))\n",
    "        \n",
    "    return pd.concat(output).drop_duplicates()\n",
    "\n",
    "def find_sources_and_targets_with_patterns(patterns: List[str], df: pd.DataFrame):\n",
    "    mask = np.zeros(kg_df.shape[0])\n",
    "    for pattern in patterns:\n",
    "        mask = mask | (df.source.str.contains(pattern)) | (df.target.str.contains(pattern))\n",
    "        \n",
    "    return df[mask]\n",
    "# Examples of how to use the function:\n",
    "# find_sources_and_targets_with_patterns(['action'], kg_df)\n",
    "# find_sources_and_targets_with_patterns(['terror'], kg_df)\n",
    "# find_sources_and_targets_with_patterns(['novel'], kg_df)\n",
    "# find_sources_and_targets_with_patterns(['director', 'campbell'], kg_df)\n",
    "\n",
    "\n",
    "def plot_graph(df: pd.DataFrame, show_edges: bool = False, figsize: Tuple[int, int] = (12, 12), use_circular: bool=True):\n",
    "    graph = nx.from_pandas_edgelist(df, \"source\", \"target\", edge_attr='edge', create_using=nx.MultiDiGraph())\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    if use_circular:\n",
    "        pos = nx.circular_layout(graph)\n",
    "    else:\n",
    "        pos = nx.kamada_kawai_layout(graph)\n",
    "        \n",
    "    nx.draw(graph, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos=pos)\n",
    "    if show_edges:\n",
    "        nx.draw_networkx_edge_labels(graph, pos=pos, font_size=8)\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "def get_top_sources_and_targets(df: pd.DataFrame, top: int = 10):\n",
    "    return (Counter(df.source) + Counter(df.target)).most_common(top)\n",
    "\n",
    "def get_top_edges(df: pd.DataFrame, top: int = 10):\n",
    "    return Counter(df.edge).most_common(top)\n",
    "\n",
    "def get_dataset_partitions_pd(df, train_split=0.8, val_split=0.10, test_split=0.10):\n",
    "       # Specify seed to always have the same split distribution between runs\n",
    "    df_sample = df.sample(frac=1, random_state=12)\n",
    "    indices_or_sections = [int(.8*len(df)), int(.9*len(df))]\n",
    "    train_ds, val_ds, test_ds = np.split(df_sample, indices_or_sections)\n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1ac9c8",
   "metadata": {},
   "source": [
    "Let's define entity extraction functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f9b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "  ## chunk 1\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "\n",
    "  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "  prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "  prefix = \"\"\n",
    "  modifier = \"\"\n",
    "\n",
    "  for tok in nlp(sent):\n",
    "    ## chunk 2\n",
    "    # if token is a punctuation mark then move on to the next token\n",
    "    if tok.dep_ != \"punct\":\n",
    "      # check: token is a compound word or not\n",
    "      if tok.dep_ == \"compound\":\n",
    "        prefix = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      # check: token is a modifier or not\n",
    "      if tok.dep_.endswith(\"mod\") == True:\n",
    "        modifier = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          modifier = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      ## chunk 3\n",
    "      if tok.dep_.find(\"subj\") == True:\n",
    "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"      \n",
    "\n",
    "      ## chunk 4\n",
    "      if tok.dep_.find(\"obj\") == True:\n",
    "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "      ## chunk 5  \n",
    "      # update variables\n",
    "      prv_tok_dep = tok.dep_\n",
    "      prv_tok_text = tok.text\n",
    "\n",
    "  return [ent1.strip(), ent2.strip()]\n",
    "\n",
    "def get_relation(sent):\n",
    "    try:\n",
    "        doc = nlp(sent)\n",
    "        \n",
    "        # Matcher class object \n",
    "        matcher = Matcher(nlp.vocab)\n",
    "\n",
    "        #define the pattern \n",
    "        pattern = [{'DEP':'ROOT'}, \n",
    "                {'DEP':'prep','OP':\"?\"},\n",
    "                {'DEP':'agent','OP':\"?\"},  \n",
    "                {'POS':'ADJ','OP':\"?\"}] \n",
    "        matcher.add(\"matching_1\", [pattern]) \n",
    "        matches = matcher(doc)\n",
    "        k = len(matches) - 1\n",
    "        span = doc[matches[k][1]:matches[k][2]] \n",
    "        \n",
    "        return(span.text)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "def get_subject_verb_object(sent):\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "  root = \"\"\n",
    "\n",
    "  for tok in nlp(sent):\n",
    "      if tok.dep_ == 'ROOT':\n",
    "        root = tok.text\n",
    "      elif tok.dep_ == \"nsubj\":\n",
    "        ent1 = tok.text\n",
    "      elif tok.dep_ == \"dobj\":\n",
    "        ent2 = tok.text\n",
    "\n",
    "      if ent1 != '' and ent2 != '' and root != '':\n",
    "        break\n",
    "\n",
    "  return [ent1, root, ent2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d28fbe5",
   "metadata": {},
   "source": [
    "Let's define functions that can be used for visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8f11a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_labeled(y_true, y_pred, CLASSES_LIST):\n",
    "    mtx = confusion_matrix(y_true, y_pred)\n",
    "    # define classes \n",
    "    classes = CLASSES_LIST\n",
    "    temp_df = pd.DataFrame(data=mtx,columns=classes)\n",
    "    temp_df.index = classes\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.heatmap(temp_df, annot=True, fmt='d', linewidths=.75,  cbar=False, ax=ax,cmap='Blues',linecolor='white')\n",
    "    #  square=True,\n",
    "    plt.ylabel('true label')\n",
    "    plt.xlabel('predicted label')\n",
    "    \n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "\n",
    "def display_training_curves(training, validation, title, subplot):\n",
    "  ax = plt.subplot(subplot)\n",
    "  ax.plot(training)\n",
    "  ax.plot(validation)\n",
    "  ax.set_title('model '+ title)\n",
    "  ax.set_ylabel(title)\n",
    "  ax.set_xlabel('epoch')\n",
    "  ax.legend(['training', 'validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649fd55a",
   "metadata": {},
   "source": [
    "Knowledge Graph Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a4811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text_sentences = [y for x in movie_df.raw_sentences for y in x]\n",
    "example_sentence = nlp(corpus_text_sentences[5])\n",
    "corpus_text_sentences[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172e6f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_pairs = [get_entities(x) for x in tqdm(corpus_text_sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b1afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d2e4a",
   "metadata": {},
   "source": [
    "Create Dataframe of Sources, Edges, and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa59c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = [get_relation(x) for x in corpus_text_sentences]   \n",
    "#extract subject and object\n",
    "source = [i[0] for i in entity_pairs]\n",
    "target = [i[1] for i in entity_pairs]\n",
    "kg_df = pd.DataFrame({'source': source, 'target': target, 'edge': relations})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5b97e",
   "metadata": {},
   "source": [
    "Knowledge Graph Preprocessing (Lowercase Transformation and Removal of Empty Spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176a4da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move everything to lower case\n",
    "kg_df.source = kg_df.source.str.lower()\n",
    "kg_df.target = kg_df.target.str.lower()\n",
    "kg_df.edge = kg_df.edge.str.lower()\n",
    "\n",
    "# Filter out empties\n",
    "kg_df = kg_df[kg_df.source != '']\n",
    "kg_df = kg_df[kg_df.target != '']\n",
    "kg_df = kg_df[kg_df.edge != ''].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d894c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_df.head(6).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4846b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8d0548",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_text_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e95666",
   "metadata": {},
   "source": [
    "Plotting Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb09698",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(kg_df, use_circular=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b1ac54",
   "metadata": {},
   "source": [
    "Plotting Knowledge Graph Subset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79420d7",
   "metadata": {},
   "source": [
    "Let's examine the subset of our knowledge graph connected to nodes that correspond to the movie title, \"Sisters.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bafe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "RELATION_TO_EXPLORE = 'sisters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2921a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(kg_df[kg_df['edge'] == RELATION_TO_EXPLORE], \n",
    "                            \"source\", \"target\", \n",
    "                            edge_attr=True, \n",
    "                            create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "pos = nx.spring_layout(G, k=0.5) # k regulates the distance between nodes\n",
    "\n",
    "nx.draw(G, with_labels=True, \n",
    "        node_color='skyblue', \n",
    "        node_size=1500, edge_cmap=plt.cm.Blues, pos=pos)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4654613",
   "metadata": {},
   "source": [
    "Plotting Knowledge Graph Subset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb00955",
   "metadata": {},
   "source": [
    "Let's examine the subset of our knowledge graph connected to nodes that correspond to one of the lead actresses, Amy Poehler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f4caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "RELATION_TO_EXPLORE = 'amy poehler'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7802b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(kg_df[kg_df['edge'] == RELATION_TO_EXPLORE], \n",
    "                            \"source\", \"target\", \n",
    "                            edge_attr=True, \n",
    "                            create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "pos = nx.spring_layout(G, k=0.5) # k regulates the distance between nodes\n",
    "\n",
    "nx.draw(G, with_labels=True, \n",
    "        node_color='skyblue', \n",
    "        node_size=1500, edge_cmap=plt.cm.Blues, pos=pos)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795cf899",
   "metadata": {},
   "source": [
    "Plotting Knowledge Graph Subset 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9756d756",
   "metadata": {},
   "source": [
    "Let's examine the subset of our knowledge graph connected to nodes that correspond to one of the lead actresses, Tina Fey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa89c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RELATION_TO_EXPLORE = 'tina fey'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f7e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(kg_df[kg_df['edge'] == RELATION_TO_EXPLORE], \n",
    "                            \"source\", \"target\", \n",
    "                            edge_attr=True, \n",
    "                            create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "pos = nx.spring_layout(G, k=0.5) # k regulates the distance between nodes\n",
    "\n",
    "nx.draw(G, with_labels=True, \n",
    "        node_color='skyblue', \n",
    "        node_size=1500, edge_cmap=plt.cm.Blues, pos=pos)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e73a93",
   "metadata": {},
   "source": [
    "## 2) Classification with Long Short-Term Memory (LSTM) Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31224f",
   "metadata": {},
   "source": [
    "Let's create Long Short-Term Memory models that are capable of classifying movie reviews by genre and by movie review sentiment (i.e. positive or negative review)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb9d2ef",
   "metadata": {},
   "source": [
    "### 2.1) Data Wrangling for LSTM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255ce42a",
   "metadata": {},
   "source": [
    "Let's conduct data wrangling and preprocessing as needed to format the corpus text for the LSTM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a31c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafull=corpus_df.copy() \n",
    "datafull.reset_index(drop=True, inplace=True)\n",
    "datafull.head(4).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb82be",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafull['Text'] = datafull['Text'].apply(lambda x :clean_doc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93551a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datafull[['Text','Genre of Movie']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753ee39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Genre of Movie'] = data['Genre of Movie'].astype(\"category\")\n",
    "data['Genre of Movie code'] = data['Genre of Movie'].cat.codes\n",
    "data['Genre of Movie code'].tail().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, targets = data['Text'], data['Genre of Movie code']                                                                         \n",
    "data[[\"Genre of Movie\",\"Genre of Movie code\"]].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89085d6",
   "metadata": {},
   "source": [
    "Let's create the training, validation, and testing datasets for construction of our LSTM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad75f2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainds, valds, testds = get_dataset_partitions_pd(data[['Text','Genre of Movie code']])\n",
    "trainds.shape, valds.shape, testds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c678062d",
   "metadata": {},
   "source": [
    "Let's convert the dataframe to a TensorFlow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9c0b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train X & y\n",
    "train_text_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(trainds['Text'].values, tf.string)\n",
    ") \n",
    "train_cat_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(trainds['Genre of Movie code'].values, tf.int64),\n",
    "  ) \n",
    "# test X & y\n",
    "test_text_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(testds['Text'].values, tf.string)\n",
    ") \n",
    "test_cat_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(testds['Genre of Movie code'].values, tf.int64),\n",
    ")\n",
    "#val X & Y\n",
    "val_text_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(valds['Text'].values, tf.string)\n",
    ") \n",
    "val_cat_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(valds['Genre of Movie code'].values, tf.int64),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4562b",
   "metadata": {},
   "source": [
    "Create Datasets (X=Preprocessed Text, Y=Encoded Categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.zip(\n",
    "    (\n",
    "            train_text_ds_raw,\n",
    "            train_cat_ds_raw\n",
    "     )\n",
    ")\n",
    "test_ds = tf.data.Dataset.zip(\n",
    "    (\n",
    "            test_text_ds_raw,\n",
    "            test_cat_ds_raw\n",
    "     )\n",
    ")\n",
    "val_ds = tf.data.Dataset.zip(\n",
    "    (\n",
    "            val_text_ds_raw,\n",
    "            val_cat_ds_raw\n",
    "     )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68da2c0",
   "metadata": {},
   "source": [
    "Create Data Pipelines (Batching, Shuffling, and Optimizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50736641",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "buffer_size=train_ds.cardinality().numpy()\n",
    "\n",
    "train_ds = train_ds.shuffle(buffer_size=buffer_size)\\\n",
    "                   .batch(batch_size=batch_size,drop_remainder=True)\\\n",
    "                   .cache()\\\n",
    "                   .prefetch(AUTOTUNE)\n",
    "\n",
    "test_ds = test_ds.shuffle(buffer_size=buffer_size)\\\n",
    "                   .batch(batch_size=batch_size,drop_remainder=True)\\\n",
    "                   .cache()\\\n",
    "                   .prefetch(AUTOTUNE)\n",
    "\n",
    "\n",
    "val_ds = val_ds.shuffle(buffer_size=buffer_size)\\\n",
    "                   .batch(batch_size=batch_size,drop_remainder=True)\\\n",
    "                   .cache()\\\n",
    "                   .prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e1d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745e9ae1",
   "metadata": {},
   "source": [
    "Create The Text Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab9975b",
   "metadata": {},
   "source": [
    "The raw text loaded by `tfds` needs to be processed before it can be used in a model. The simplest way to process text for training is using the `experimental.preprocessing.TextVectorization` layer. This layer has many capabilities, but this tutorial sticks to the default behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fa2fcb",
   "metadata": {},
   "source": [
    "Create the layer, and pass the dataset's text to the layer's `.adapt` method:\n",
    "The processing of each sample contains the following steps:\n",
    "\n",
    "    standardize each sample (usually lowercasing + punctuation stripping)\n",
    "    split each sample into substrings (usually words)\n",
    "    recombine substrings into tokens (usually ngrams)\n",
    "    index tokens (associate a unique int value with each token)\n",
    "    transform each sample using this index, either into a vector of ints or a dense float vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436951c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE=5000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE, standardize=\"lower_and_strip_punctuation\", pad_to_max_tokens= True)\n",
    "encoder.adapt(train_ds.map(lambda text, label: text), batch_size= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cd1ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c535e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f2499",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af2cf81",
   "metadata": {},
   "source": [
    "View Examples of Encoded Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a3d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example = encoder('encanto we dont talk about bruno no no').numpy()\n",
    "encoded_example[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d8733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoder.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb68d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder(data['Text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b150e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example, label in train_ds.take(1):\n",
    "  print('texts: ', example.numpy()[:1])\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5181cd",
   "metadata": {},
   "source": [
    "### 2.2) Genre Classification with LSTM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9c036",
   "metadata": {},
   "source": [
    "Let's first create LSTM models that are capable of classifying movie reviews into one of four genres: 1) horror, 2) science fiction, 3) comedy, or 4) action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc84a0",
   "metadata": {},
   "source": [
    "#### 2.2.1) Genre Classification LSTM Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44158aa",
   "metadata": {},
   "source": [
    "Build the LSTM Model. For more information about layers in keras, refer to this resource: https://www.tensorflow.org/api_docs/python/tf/keras/layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c35081",
   "metadata": {},
   "outputs": [],
   "source": [
    "k.clear_session()\n",
    "num_classes=4\n",
    "model=tf.keras.Sequential([encoder\n",
    "   ,tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True)\n",
    "   ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True, dropout=0.3))\n",
    "   ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,dropout=0.3))\n",
    "   ,tf.keras.layers.Dense(64, activation='relu')\n",
    "   ,tf.keras.layers.Dense(num_classes,activation='softmax')  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= tf.keras.optimizers.Adam( )\n",
    "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() \n",
    "              ,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd17ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(train_ds\n",
    "         ,epochs=200\n",
    "         ,validation_data=val_ds\n",
    "         ,validation_steps=3\n",
    "         ,callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bf94d4",
   "metadata": {},
   "source": [
    "View a Summary of the LSTM Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd1a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9653956e",
   "metadata": {},
   "source": [
    "Assess Model's Performance Using Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5315e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5b88de",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ddcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345dd47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f03360",
   "metadata": {},
   "source": [
    "Assess the Model's Performance Using Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031916a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2 = model.predict(test_ds)\n",
    "y_pred2 = np.argmax(preds2, axis=1)\n",
    "y2 = np.concatenate([y for x, y in test_ds], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a03f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES_LIST = ['Action','Comedy','Horror','Sci Fi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83ccad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix_labeled(y2,y_pred2, CLASSES_LIST=CLASSES_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b3acac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sns.light_palette((260, 75, 60), input=\"husl\", as_cmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af801020",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(preds2[0:15]\n",
    "                  ,columns = CLASSES_LIST).T\n",
    "df2.style.format(\"{:.2%}\").background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9cba9a",
   "metadata": {},
   "source": [
    "#### 2.2.2) Genre Classification LSTM Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb0d40a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380efc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691d6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e26d6c48",
   "metadata": {},
   "source": [
    "#### 2.2.3) Genre Classification LSTM Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ace5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a024cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d19a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73933cb0",
   "metadata": {},
   "source": [
    "#### 2.2.4) Genre Classification LSTM Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a57efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36819b77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb4365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2defd0e",
   "metadata": {},
   "source": [
    "### 2.3) Sentiment Analysis and Classification with LSTM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21d177",
   "metadata": {},
   "source": [
    "Let's now turn our attention toward creating LSTM models capable of classifying movie reviews as having either positive or negative sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5b36f8",
   "metadata": {},
   "source": [
    "#### 2.3.1) Sentiment Classification LSTM Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f125915b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f167560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87115ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "825cd4aa",
   "metadata": {},
   "source": [
    "#### 2.3.2) Sentiment Classification LSTM Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8bd262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d3e52e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7d0cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48e8af3c",
   "metadata": {},
   "source": [
    "#### 2.3.3) Sentiment Classification LSTM Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb525ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fffaba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964efd06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87c77442",
   "metadata": {},
   "source": [
    "#### Sentiment Classification LSTM Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b494bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225b55d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc57a137",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
