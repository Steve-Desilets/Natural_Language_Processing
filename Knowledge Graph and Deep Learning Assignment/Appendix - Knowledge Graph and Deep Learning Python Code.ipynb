{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b6fcb16",
   "metadata": {},
   "source": [
    "# Appendix - Knowledge Graph and Deep Learning Python Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce8389",
   "metadata": {},
   "source": [
    "## 1) Knowledge Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f466a",
   "metadata": {},
   "source": [
    "Let's leverage the reviews of the movie \"Sisters\" in our corpus to generate knowledge graphs that can visualize relationships between named entities in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34baa0b",
   "metadata": {},
   "source": [
    "### 1.1) Loading the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db3693c",
   "metadata": {},
   "source": [
    "First, let's load the corpus into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef0d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "tf.random.set_seed(2022)\n",
    "import tensorflow.keras.backend as k\n",
    "\n",
    "from typing import List, Callable, Dict, Tuple, Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf948b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires Restart of Runtime after Installation\n",
    "\n",
    "#!python -m spacy download en_core_web_lg -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed22ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this once, they will be downloaded.\n",
    "nltk.download('stopwords',quiet=True)\n",
    "nltk.download('wordnet',quiet=True)\n",
    "nltk.download('punkt',quiet=True)\n",
    "nltk.download('omw-1.4',quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87325a8",
   "metadata": {},
   "source": [
    "\n",
    "Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Target audience is the natural language processing (NLP) and information retrieval (IR) community: https://pypi.org/project/gensim/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13018254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "#pkg_resources.require(\"gensim<=3.8.3\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a0e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Genism Version: \", gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f546963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Warning Messages\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58fe039",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_PATH=\\\n",
    "'https://raw.githubusercontent.com/barrycforever/MSDS_453_Public/main/MSDS453_ClassCorpus/MSDS453_QA_20220906.csv'\n",
    "corpus_df = get_corpus_df(CORPUS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815eaeb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2414fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9c6e3ff",
   "metadata": {},
   "source": [
    "### 1.2) Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c496d7af",
   "metadata": {},
   "source": [
    "Having loaded the appropriate Python libraries and loaded the corpus of movie reviews, let's conduct some initial exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae1c281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "corpus_df['raw_sentences'] = corpus_df.Text.apply(get_sentences)\n",
    "corpus_df.head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e10b3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25618cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3990dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d85688b0",
   "metadata": {},
   "source": [
    "### 1.3) Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4b901a",
   "metadata": {},
   "source": [
    "Let's conduct data wrangling to prepare the text as needed prior to the creation of our knowledge graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee07ba9d",
   "metadata": {},
   "source": [
    "#### 1.3.1) Data Wrangling and Vectorization Method 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c31684",
   "metadata": {},
   "source": [
    "Let's define data wrangling utility functions that we can leverage on our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e7ee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_movie_descriptor(data: pd.DataFrame, corpus_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Adds \"Movie Description\" to the supplied dataframe, in the form {Genre}_{P|N}_{Movie Title}_{DocID}\n",
    "    \"\"\"\n",
    "    review = np.where(corpus_df['Review Type (pos or neg)'] == 'Positive', 'P', 'N')\n",
    "    data['Descriptor'] = corpus_df['Genre of Movie'] + '_' + corpus_df['Movie Title'] + '_' + review + '_' + corpus_df['Doc_ID'].astype(str)\n",
    "    \n",
    "def get_corpus_df(path: str) -> pd.DataFrame:\n",
    "    data = pd.read_csv(path,encoding=\"utf-8\")\n",
    "    add_movie_descriptor(data, data)\n",
    "    sorted_data = data.sort_values(['Descriptor'])\n",
    "    indexed_data = sorted_data.set_index(['Doc_ID'])\n",
    "    indexed_data['Doc_ID'] = indexed_data.index\n",
    "    return indexed_data\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    return re.sub('[^a-zA-Z]', '', str(text))\n",
    "\n",
    "def remove_tags(text: str) -> str:    \n",
    "    return re.sub('&lt;/?.*?&gt;', '', text)\n",
    "\n",
    "def remove_special_chars_and_digits(text: str) -> str:\n",
    "    return re.sub('(\\\\d|\\\\W)+', '', text)\n",
    "\n",
    "def get_sentences(text: str) -> List[str]:\n",
    "    return [str(x) for x in nlp(text).sents]\n",
    "\n",
    "def get_coref_resolved_sentences(text: str) -> List[str]:\n",
    "    return [str(x) for x in nlp(text).sents]\n",
    "\n",
    "def get_lemmas(text: str, stopwords: Set[str]) -> List[str]:\n",
    "    initial = [remove_tags(remove_special_chars_and_digits(remove_punctuation(x.lemma_.lower()))) for x in nlp(text)]\n",
    "    return [x for x in initial if x not in stopwords]\n",
    "\n",
    "def lemmatize_sentence(text: str, stopwords: Set[str]) -> str:\n",
    "    return ' '.join(get_lemmas(text, stopwords))\n",
    "\n",
    "def clean_doc(doc): \n",
    "    #doc = remove_punctuation(doc)\n",
    "    doc= ' '.join(remove_stop_words(doc))\n",
    "    doc = apply_lemmatization(doc)\n",
    "    return doc\n",
    "\n",
    "def remove_stop_words(in_text):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(in_text)  \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    return filtered_sentence\n",
    "\n",
    "def apply_lemmatization(in_text):\n",
    "    # Lemmatization\n",
    "    lem = WordNetLemmatizer()\n",
    "    word_list = nltk.word_tokenize(in_text)\n",
    "    output = ' '.join([lem.lemmatize(w) for w in word_list])\n",
    "    return output\n",
    "\n",
    "def counter_word(text):\n",
    "  count=Counter()\n",
    "  for i in text.values:\n",
    "    for word in i.split():\n",
    "      count[word]=+1\n",
    "  return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e54ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33081a07",
   "metadata": {},
   "source": [
    "#### 1.3.2) Data Wrangling and Vectorization Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eea67a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682dfc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28018eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1260cf22",
   "metadata": {},
   "source": [
    "#### 1.3.3) Data Wrangling and Vectorization Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1202daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec561f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54db96d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bb5b49d",
   "metadata": {},
   "source": [
    "### 1.4) Knowledge Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fff971",
   "metadata": {},
   "source": [
    "Let's define knowledge context graph functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94661b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_edges(map_to: str, map_from: Set[str], df: pd.DataFrame):\n",
    "    print(f'Before mapping {\", \".join(map_from)} -> {map_to}: {sum(df.edge == map_to)}')\n",
    "    df['edge'] = np.where(kg_df.edge.isin(map_from), map_to, kg_df.edge)\n",
    "    print(f'After mapping {\", \".join(map_from)} -> {map_to}: {sum(df.edge == map_to)}')\n",
    "    \n",
    "def map_sources_and_targets(map_to: str, map_from: Set[str], df: pd.DataFrame):\n",
    "    before = sum(df.source == map_to) + sum(df.target == map_to)\n",
    "    print(f'Before mapping {\", \".join(map_from)} -> {map_to}: {before}')\n",
    "    \n",
    "    df['source'] = np.where(kg_df.source.isin(map_from), map_to, kg_df.source)\n",
    "    df['target'] = np.where(kg_df.target.isin(map_from), map_to, kg_df.target)\n",
    "    \n",
    "    after = sum(df.source == map_to) + sum(df.target == map_to)\n",
    "    print(f'After mapping {\", \".join(map_from)} -> {map_to}: {after}')\n",
    "    \n",
    "def get_neighborhood(sources: Set[str], edge_types: Set[str], depth: int, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    output = []\n",
    "    \n",
    "    for d in range(depth):\n",
    "        if edge_types is not None:\n",
    "            rows = df[(df.edge.isin(edge_types)) & ((df.source.isin(sources)) | (df.target.isin(sources)))].copy()\n",
    "        else:\n",
    "            rows = df[(df.source.isin(sources)) | (df.target.isin(sources))].copy()\n",
    "            \n",
    "        output.append(rows)\n",
    "        sources = set(rows.target).union(set(rows.source))\n",
    "        \n",
    "    return pd.concat(output).drop_duplicates()\n",
    "\n",
    "def find_sources_and_targets_with_patterns(patterns: List[str], df: pd.DataFrame):\n",
    "    mask = np.zeros(kg_df.shape[0])\n",
    "    for pattern in patterns:\n",
    "        mask = mask | (df.source.str.contains(pattern)) | (df.target.str.contains(pattern))\n",
    "        \n",
    "    return df[mask]\n",
    "# Examples of how to use the function:\n",
    "# find_sources_and_targets_with_patterns(['action'], kg_df)\n",
    "# find_sources_and_targets_with_patterns(['terror'], kg_df)\n",
    "# find_sources_and_targets_with_patterns(['novel'], kg_df)\n",
    "# find_sources_and_targets_with_patterns(['director', 'campbell'], kg_df)\n",
    "\n",
    "\n",
    "def plot_graph(df: pd.DataFrame, show_edges: bool = False, figsize: Tuple[int, int] = (12, 12), use_circular: bool=True):\n",
    "    graph = nx.from_pandas_edgelist(df, \"source\", \"target\", edge_attr='edge', create_using=nx.MultiDiGraph())\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    if use_circular:\n",
    "        pos = nx.circular_layout(graph)\n",
    "    else:\n",
    "        pos = nx.kamada_kawai_layout(graph)\n",
    "        \n",
    "    nx.draw(graph, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos=pos)\n",
    "    if show_edges:\n",
    "        nx.draw_networkx_edge_labels(graph, pos=pos, font_size=8)\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "def get_top_sources_and_targets(df: pd.DataFrame, top: int = 10):\n",
    "    return (Counter(df.source) + Counter(df.target)).most_common(top)\n",
    "\n",
    "def get_top_edges(df: pd.DataFrame, top: int = 10):\n",
    "    return Counter(df.edge).most_common(top)\n",
    "\n",
    "def get_dataset_partitions_pd(df, train_split=0.8, val_split=0.10, test_split=0.10):\n",
    "       # Specify seed to always have the same split distribution between runs\n",
    "    df_sample = df.sample(frac=1, random_state=12)\n",
    "    indices_or_sections = [int(.8*len(df)), int(.9*len(df))]\n",
    "    train_ds, val_ds, test_ds = np.split(df_sample, indices_or_sections)\n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396c973c",
   "metadata": {},
   "source": [
    "Let's define entity extraction functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f791ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "  ## chunk 1\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "\n",
    "  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "  prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "  prefix = \"\"\n",
    "  modifier = \"\"\n",
    "\n",
    "  for tok in nlp(sent):\n",
    "    ## chunk 2\n",
    "    # if token is a punctuation mark then move on to the next token\n",
    "    if tok.dep_ != \"punct\":\n",
    "      # check: token is a compound word or not\n",
    "      if tok.dep_ == \"compound\":\n",
    "        prefix = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      # check: token is a modifier or not\n",
    "      if tok.dep_.endswith(\"mod\") == True:\n",
    "        modifier = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          modifier = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      ## chunk 3\n",
    "      if tok.dep_.find(\"subj\") == True:\n",
    "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"      \n",
    "\n",
    "      ## chunk 4\n",
    "      if tok.dep_.find(\"obj\") == True:\n",
    "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "      ## chunk 5  \n",
    "      # update variables\n",
    "      prv_tok_dep = tok.dep_\n",
    "      prv_tok_text = tok.text\n",
    "\n",
    "  return [ent1.strip(), ent2.strip()]\n",
    "\n",
    "def get_relation(sent):\n",
    "    try:\n",
    "        doc = nlp(sent)\n",
    "        \n",
    "        # Matcher class object \n",
    "        matcher = Matcher(nlp.vocab)\n",
    "\n",
    "        #define the pattern \n",
    "        pattern = [{'DEP':'ROOT'}, \n",
    "                {'DEP':'prep','OP':\"?\"},\n",
    "                {'DEP':'agent','OP':\"?\"},  \n",
    "                {'POS':'ADJ','OP':\"?\"}] \n",
    "        matcher.add(\"matching_1\", [pattern]) \n",
    "        matches = matcher(doc)\n",
    "        k = len(matches) - 1\n",
    "        span = doc[matches[k][1]:matches[k][2]] \n",
    "        \n",
    "        return(span.text)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "def get_subject_verb_object(sent):\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "  root = \"\"\n",
    "\n",
    "  for tok in nlp(sent):\n",
    "      if tok.dep_ == 'ROOT':\n",
    "        root = tok.text\n",
    "      elif tok.dep_ == \"nsubj\":\n",
    "        ent1 = tok.text\n",
    "      elif tok.dep_ == \"dobj\":\n",
    "        ent2 = tok.text\n",
    "\n",
    "      if ent1 != '' and ent2 != '' and root != '':\n",
    "        break\n",
    "\n",
    "  return [ent1, root, ent2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ddbe3e",
   "metadata": {},
   "source": [
    "Let's define functions that can be used for visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f330660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_labeled(y_true, y_pred, CLASSES_LIST):\n",
    "    mtx = confusion_matrix(y_true, y_pred)\n",
    "    # define classes \n",
    "    classes = CLASSES_LIST\n",
    "    temp_df = pd.DataFrame(data=mtx,columns=classes)\n",
    "    temp_df.index = classes\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.heatmap(temp_df, annot=True, fmt='d', linewidths=.75,  cbar=False, ax=ax,cmap='Blues',linecolor='white')\n",
    "    #  square=True,\n",
    "    plt.ylabel('true label')\n",
    "    plt.xlabel('predicted label')\n",
    "    \n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "\n",
    "def display_training_curves(training, validation, title, subplot):\n",
    "  ax = plt.subplot(subplot)\n",
    "  ax.plot(training)\n",
    "  ax.plot(validation)\n",
    "  ax.set_title('model '+ title)\n",
    "  ax.set_ylabel(title)\n",
    "  ax.set_xlabel('epoch')\n",
    "  ax.legend(['training', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e4f333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb74aa63",
   "metadata": {},
   "source": [
    "## 2) Classification with Long Short-Term Memory (LSTM) Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1318868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88579f49",
   "metadata": {},
   "source": [
    "### 2.1) Genre Classification with LSTM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4bcafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f38ecda",
   "metadata": {},
   "source": [
    "#### 2.1.1) Genre Classification LSTM Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d79553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8fcad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8791c24d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b889ce11",
   "metadata": {},
   "source": [
    "#### 2.1.2) Genre Classification LSTM Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755da002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67bcd2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeec3adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa32f51c",
   "metadata": {},
   "source": [
    "#### 2.1.3) Genre Classification LSTM Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2030551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c65ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db6d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f46c96bd",
   "metadata": {},
   "source": [
    "### 2.2) Sentiment Analysis and Classification with LSTM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffd1dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbcd0879",
   "metadata": {},
   "source": [
    "#### 2.2.1) Sentiment Classification LSTM Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc4bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda7df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36f55ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "442abd40",
   "metadata": {},
   "source": [
    "#### 2.2.2) Sentiment Classification LSTM Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82942d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f9830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa25f4df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "682f054d",
   "metadata": {},
   "source": [
    "#### 2.2.3) Sentiment Classification LSTM Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e3c909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a30cd58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4ac6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
