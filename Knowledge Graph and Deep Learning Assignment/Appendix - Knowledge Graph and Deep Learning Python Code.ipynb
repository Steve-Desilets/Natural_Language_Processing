{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b6fcb16",
   "metadata": {},
   "source": [
    "# Appendix - Knowledge Graph and Deep Learning Python Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e1b9bb",
   "metadata": {},
   "source": [
    "## 1) Knowledge Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab2395",
   "metadata": {},
   "source": [
    "Let's leverage the reviews of the movie \"Sisters\" in our corpus to generate knowledge graphs that can visualize relationships between named entities in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639fdc1a",
   "metadata": {},
   "source": [
    "### 1.1) Loading the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d7d69c",
   "metadata": {},
   "source": [
    "First, let's load the corpus into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f7ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "tf.random.set_seed(2022)\n",
    "import tensorflow.keras.backend as k\n",
    "\n",
    "from typing import List, Callable, Dict, Tuple, Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5085a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires Restart of Runtime after Installation\n",
    "\n",
    "#!python -m spacy download en_core_web_lg -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9c7dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd72691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this once, they will be downloaded.\n",
    "nltk.download('stopwords',quiet=True)\n",
    "nltk.download('wordnet',quiet=True)\n",
    "nltk.download('punkt',quiet=True)\n",
    "nltk.download('omw-1.4',quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49830a4",
   "metadata": {},
   "source": [
    "\n",
    "Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Target audience is the natural language processing (NLP) and information retrieval (IR) community: https://pypi.org/project/gensim/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b3abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "#pkg_resources.require(\"gensim<=3.8.3\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7597e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Genism Version: \", gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7de2db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Warning Messages\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bf7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_PATH=\\\n",
    "'https://raw.githubusercontent.com/barrycforever/MSDS_453_Public/main/MSDS453_ClassCorpus/MSDS453_QA_20220906.csv'\n",
    "corpus_df = get_corpus_df(CORPUS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfafbae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef17f510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33f9bf7c",
   "metadata": {},
   "source": [
    "### 1.2) Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2300ae4",
   "metadata": {},
   "source": [
    "Having loaded the appropriate Python libraries and loaded the corpus of movie reviews, let's conduct some initial exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2cd71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "corpus_df['raw_sentences'] = corpus_df.Text.apply(get_sentences)\n",
    "corpus_df.head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab1f038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfb4815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3190a707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e48c8d9",
   "metadata": {},
   "source": [
    "### 1.3) Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1385dba",
   "metadata": {},
   "source": [
    "Let's conduct data wrangling to prepare the text as needed prior to the creation of our knowledge graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e92bdb",
   "metadata": {},
   "source": [
    "#### 1.3.1) Data Wrangling and Vectorization Method 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449e5f93",
   "metadata": {},
   "source": [
    "Let's define data wrangling utility functions that we can leverage on our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2206bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_movie_descriptor(data: pd.DataFrame, corpus_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Adds \"Movie Description\" to the supplied dataframe, in the form {Genre}_{P|N}_{Movie Title}_{DocID}\n",
    "    \"\"\"\n",
    "    review = np.where(corpus_df['Review Type (pos or neg)'] == 'Positive', 'P', 'N')\n",
    "    data['Descriptor'] = corpus_df['Genre of Movie'] + '_' + corpus_df['Movie Title'] + '_' + review + '_' + corpus_df['Doc_ID'].astype(str)\n",
    "    \n",
    "def get_corpus_df(path: str) -> pd.DataFrame:\n",
    "    data = pd.read_csv(path,encoding=\"utf-8\")\n",
    "    add_movie_descriptor(data, data)\n",
    "    sorted_data = data.sort_values(['Descriptor'])\n",
    "    indexed_data = sorted_data.set_index(['Doc_ID'])\n",
    "    indexed_data['Doc_ID'] = indexed_data.index\n",
    "    return indexed_data\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    return re.sub('[^a-zA-Z]', '', str(text))\n",
    "\n",
    "def remove_tags(text: str) -> str:    \n",
    "    return re.sub('&lt;/?.*?&gt;', '', text)\n",
    "\n",
    "def remove_special_chars_and_digits(text: str) -> str:\n",
    "    return re.sub('(\\\\d|\\\\W)+', '', text)\n",
    "\n",
    "def get_sentences(text: str) -> List[str]:\n",
    "    return [str(x) for x in nlp(text).sents]\n",
    "\n",
    "def get_coref_resolved_sentences(text: str) -> List[str]:\n",
    "    return [str(x) for x in nlp(text).sents]\n",
    "\n",
    "def get_lemmas(text: str, stopwords: Set[str]) -> List[str]:\n",
    "    initial = [remove_tags(remove_special_chars_and_digits(remove_punctuation(x.lemma_.lower()))) for x in nlp(text)]\n",
    "    return [x for x in initial if x not in stopwords]\n",
    "\n",
    "def lemmatize_sentence(text: str, stopwords: Set[str]) -> str:\n",
    "    return ' '.join(get_lemmas(text, stopwords))\n",
    "\n",
    "def clean_doc(doc): \n",
    "    #doc = remove_punctuation(doc)\n",
    "    doc= ' '.join(remove_stop_words(doc))\n",
    "    doc = apply_lemmatization(doc)\n",
    "    return doc\n",
    "\n",
    "def remove_stop_words(in_text):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(in_text)  \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    return filtered_sentence\n",
    "\n",
    "def apply_lemmatization(in_text):\n",
    "    # Lemmatization\n",
    "    lem = WordNetLemmatizer()\n",
    "    word_list = nltk.word_tokenize(in_text)\n",
    "    output = ' '.join([lem.lemmatize(w) for w in word_list])\n",
    "    return output\n",
    "\n",
    "def counter_word(text):\n",
    "  count=Counter()\n",
    "  for i in text.values:\n",
    "    for word in i.split():\n",
    "      count[word]=+1\n",
    "  return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ef3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88d130a9",
   "metadata": {},
   "source": [
    "#### 1.3.2) Data Wrangling and Vectorization Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e170c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2a4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680277ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6ee9908",
   "metadata": {},
   "source": [
    "#### 1.3.3) Data Wrangling and Vectorization Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c81325a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8d37a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a796e472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "668e5e0b",
   "metadata": {},
   "source": [
    "### 1.4) Knowledge Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796eaf20",
   "metadata": {},
   "source": [
    "Let's define knowledge context graph functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e2ac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_edges(map_to: str, map_from: Set[str], df: pd.DataFrame):\n",
    "    print(f'Before mapping {\", \".join(map_from)} -> {map_to}: {sum(df.edge == map_to)}')\n",
    "    df['edge'] = np.where(kg_df.edge.isin(map_from), map_to, kg_df.edge)\n",
    "    print(f'After mapping {\", \".join(map_from)} -> {map_to}: {sum(df.edge == map_to)}')\n",
    "    \n",
    "def map_sources_and_targets(map_to: str, map_from: Set[str], df: pd.DataFrame):\n",
    "    before = sum(df.source == map_to) + sum(df.target == map_to)\n",
    "    print(f'Before mapping {\", \".join(map_from)} -> {map_to}: {before}')\n",
    "    \n",
    "    df['source'] = np.where(kg_df.source.isin(map_from), map_to, kg_df.source)\n",
    "    df['target'] = np.where(kg_df.target.isin(map_from), map_to, kg_df.target)\n",
    "    \n",
    "    after = sum(df.source == map_to) + sum(df.target == map_to)\n",
    "    print(f'After mapping {\", \".join(map_from)} -> {map_to}: {after}')\n",
    "    \n",
    "def get_neighborhood(sources: Set[str], edge_types: Set[str], depth: int, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    output = []\n",
    "    \n",
    "    for d in range(depth):\n",
    "        if edge_types is not None:\n",
    "            rows = df[(df.edge.isin(edge_types)) & ((df.source.isin(sources)) | (df.target.isin(sources)))].copy()\n",
    "        else:\n",
    "            rows = df[(df.source.isin(sources)) | (df.target.isin(sources))].copy()\n",
    "            \n",
    "        output.append(rows)\n",
    "        sources = set(rows.target).union(set(rows.source))\n",
    "        \n",
    "    return pd.concat(output).drop_duplicates()\n",
    "\n",
    "def find_sources_and_targets_with_patterns(patterns: List[str], df: pd.DataFrame):\n",
    "    mask = np.zeros(kg_df.shape[0])\n",
    "    for pattern in patterns:\n",
    "        mask = mask | (df.source.str.contains(pattern)) | (df.target.str.contains(pattern))\n",
    "        \n",
    "    return df[mask]\n",
    "# Examples of how to use the function:\n",
    "# find_sources_and_targets_with_patterns(['action'], kg_df)\n",
    "# find_sources_and_targets_with_patterns(['terror'], kg_df)\n",
    "# find_sources_and_targets_with_patterns(['novel'], kg_df)\n",
    "# find_sources_and_targets_with_patterns(['director', 'campbell'], kg_df)\n",
    "\n",
    "\n",
    "def plot_graph(df: pd.DataFrame, show_edges: bool = False, figsize: Tuple[int, int] = (12, 12), use_circular: bool=True):\n",
    "    graph = nx.from_pandas_edgelist(df, \"source\", \"target\", edge_attr='edge', create_using=nx.MultiDiGraph())\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    if use_circular:\n",
    "        pos = nx.circular_layout(graph)\n",
    "    else:\n",
    "        pos = nx.kamada_kawai_layout(graph)\n",
    "        \n",
    "    nx.draw(graph, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos=pos)\n",
    "    if show_edges:\n",
    "        nx.draw_networkx_edge_labels(graph, pos=pos, font_size=8)\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "def get_top_sources_and_targets(df: pd.DataFrame, top: int = 10):\n",
    "    return (Counter(df.source) + Counter(df.target)).most_common(top)\n",
    "\n",
    "def get_top_edges(df: pd.DataFrame, top: int = 10):\n",
    "    return Counter(df.edge).most_common(top)\n",
    "\n",
    "def get_dataset_partitions_pd(df, train_split=0.8, val_split=0.10, test_split=0.10):\n",
    "       # Specify seed to always have the same split distribution between runs\n",
    "    df_sample = df.sample(frac=1, random_state=12)\n",
    "    indices_or_sections = [int(.8*len(df)), int(.9*len(df))]\n",
    "    train_ds, val_ds, test_ds = np.split(df_sample, indices_or_sections)\n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ed09ec",
   "metadata": {},
   "source": [
    "Let's define entity extraction functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd7a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "  ## chunk 1\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "\n",
    "  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "  prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "  prefix = \"\"\n",
    "  modifier = \"\"\n",
    "\n",
    "  for tok in nlp(sent):\n",
    "    ## chunk 2\n",
    "    # if token is a punctuation mark then move on to the next token\n",
    "    if tok.dep_ != \"punct\":\n",
    "      # check: token is a compound word or not\n",
    "      if tok.dep_ == \"compound\":\n",
    "        prefix = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      # check: token is a modifier or not\n",
    "      if tok.dep_.endswith(\"mod\") == True:\n",
    "        modifier = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          modifier = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      ## chunk 3\n",
    "      if tok.dep_.find(\"subj\") == True:\n",
    "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"      \n",
    "\n",
    "      ## chunk 4\n",
    "      if tok.dep_.find(\"obj\") == True:\n",
    "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "      ## chunk 5  \n",
    "      # update variables\n",
    "      prv_tok_dep = tok.dep_\n",
    "      prv_tok_text = tok.text\n",
    "\n",
    "  return [ent1.strip(), ent2.strip()]\n",
    "\n",
    "def get_relation(sent):\n",
    "    try:\n",
    "        doc = nlp(sent)\n",
    "        \n",
    "        # Matcher class object \n",
    "        matcher = Matcher(nlp.vocab)\n",
    "\n",
    "        #define the pattern \n",
    "        pattern = [{'DEP':'ROOT'}, \n",
    "                {'DEP':'prep','OP':\"?\"},\n",
    "                {'DEP':'agent','OP':\"?\"},  \n",
    "                {'POS':'ADJ','OP':\"?\"}] \n",
    "        matcher.add(\"matching_1\", [pattern]) \n",
    "        matches = matcher(doc)\n",
    "        k = len(matches) - 1\n",
    "        span = doc[matches[k][1]:matches[k][2]] \n",
    "        \n",
    "        return(span.text)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "def get_subject_verb_object(sent):\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "  root = \"\"\n",
    "\n",
    "  for tok in nlp(sent):\n",
    "      if tok.dep_ == 'ROOT':\n",
    "        root = tok.text\n",
    "      elif tok.dep_ == \"nsubj\":\n",
    "        ent1 = tok.text\n",
    "      elif tok.dep_ == \"dobj\":\n",
    "        ent2 = tok.text\n",
    "\n",
    "      if ent1 != '' and ent2 != '' and root != '':\n",
    "        break\n",
    "\n",
    "  return [ent1, root, ent2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa3358d",
   "metadata": {},
   "source": [
    "Let's define functions that can be used for visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e8c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_labeled(y_true, y_pred, CLASSES_LIST):\n",
    "    mtx = confusion_matrix(y_true, y_pred)\n",
    "    # define classes \n",
    "    classes = CLASSES_LIST\n",
    "    temp_df = pd.DataFrame(data=mtx,columns=classes)\n",
    "    temp_df.index = classes\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.heatmap(temp_df, annot=True, fmt='d', linewidths=.75,  cbar=False, ax=ax,cmap='Blues',linecolor='white')\n",
    "    #  square=True,\n",
    "    plt.ylabel('true label')\n",
    "    plt.xlabel('predicted label')\n",
    "    \n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "\n",
    "def display_training_curves(training, validation, title, subplot):\n",
    "  ax = plt.subplot(subplot)\n",
    "  ax.plot(training)\n",
    "  ax.plot(validation)\n",
    "  ax.set_title('model '+ title)\n",
    "  ax.set_ylabel(title)\n",
    "  ax.set_xlabel('epoch')\n",
    "  ax.legend(['training', 'validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d06627b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133bfb8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a22c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20e23781",
   "metadata": {},
   "source": [
    "## 2) Classification with Long Short-Term Memory (LSTM) Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8282369",
   "metadata": {},
   "source": [
    "Let's create Long Short-Term Memory models that are capable of classifying movie reviews by genre and by movie review sentiment (i.e. positive or negative review)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cefe3a",
   "metadata": {},
   "source": [
    "### 2.1) Data Wrangling for LSTM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437ed555",
   "metadata": {},
   "source": [
    "Let's conduct data wrangling and preprocessing as needed to format the corpus text for the LSTM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbf6663",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafull=corpus_df.copy() \n",
    "datafull.reset_index(drop=True, inplace=True)\n",
    "datafull.head(4).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6bc53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafull['Text'] = datafull['Text'].apply(lambda x :clean_doc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df20990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datafull[['Text','Genre of Movie']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b153952",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Genre of Movie'] = data['Genre of Movie'].astype(\"category\")\n",
    "data['Genre of Movie code'] = data['Genre of Movie'].cat.codes\n",
    "data['Genre of Movie code'].tail().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf5ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, targets = data['Text'], data['Genre of Movie code']                                                                         \n",
    "data[[\"Genre of Movie\",\"Genre of Movie code\"]].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272109a5",
   "metadata": {},
   "source": [
    "Let's create the training, validation, and testing datasets for construction of our LSTM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b32396",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainds, valds, testds = get_dataset_partitions_pd(data[['Text','Genre of Movie code']])\n",
    "trainds.shape, valds.shape, testds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c9023",
   "metadata": {},
   "source": [
    "Let's convert the dataframe to a TensorFlow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179b386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train X & y\n",
    "train_text_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(trainds['Text'].values, tf.string)\n",
    ") \n",
    "train_cat_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(trainds['Genre of Movie code'].values, tf.int64),\n",
    "  ) \n",
    "# test X & y\n",
    "test_text_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(testds['Text'].values, tf.string)\n",
    ") \n",
    "test_cat_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(testds['Genre of Movie code'].values, tf.int64),\n",
    ")\n",
    "#val X & Y\n",
    "val_text_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(valds['Text'].values, tf.string)\n",
    ") \n",
    "val_cat_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(valds['Genre of Movie code'].values, tf.int64),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3883232",
   "metadata": {},
   "source": [
    "Create Datasets (X=Preprocessed Text, Y=Encoded Categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a3c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.zip(\n",
    "    (\n",
    "            train_text_ds_raw,\n",
    "            train_cat_ds_raw\n",
    "     )\n",
    ")\n",
    "test_ds = tf.data.Dataset.zip(\n",
    "    (\n",
    "            test_text_ds_raw,\n",
    "            test_cat_ds_raw\n",
    "     )\n",
    ")\n",
    "val_ds = tf.data.Dataset.zip(\n",
    "    (\n",
    "            val_text_ds_raw,\n",
    "            val_cat_ds_raw\n",
    "     )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3361d4c",
   "metadata": {},
   "source": [
    "Create Data Pipelines (Batching, Shuffling, and Optimizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00909176",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "buffer_size=train_ds.cardinality().numpy()\n",
    "\n",
    "train_ds = train_ds.shuffle(buffer_size=buffer_size)\\\n",
    "                   .batch(batch_size=batch_size,drop_remainder=True)\\\n",
    "                   .cache()\\\n",
    "                   .prefetch(AUTOTUNE)\n",
    "\n",
    "test_ds = test_ds.shuffle(buffer_size=buffer_size)\\\n",
    "                   .batch(batch_size=batch_size,drop_remainder=True)\\\n",
    "                   .cache()\\\n",
    "                   .prefetch(AUTOTUNE)\n",
    "\n",
    "\n",
    "val_ds = val_ds.shuffle(buffer_size=buffer_size)\\\n",
    "                   .batch(batch_size=batch_size,drop_remainder=True)\\\n",
    "                   .cache()\\\n",
    "                   .prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20d88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849ecf0d",
   "metadata": {},
   "source": [
    "Create The Text Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d72066d",
   "metadata": {},
   "source": [
    "The raw text loaded by `tfds` needs to be processed before it can be used in a model. The simplest way to process text for training is using the `experimental.preprocessing.TextVectorization` layer. This layer has many capabilities, but this tutorial sticks to the default behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e716902a",
   "metadata": {},
   "source": [
    "Create the layer, and pass the dataset's text to the layer's `.adapt` method:\n",
    "The processing of each sample contains the following steps:\n",
    "\n",
    "    standardize each sample (usually lowercasing + punctuation stripping)\n",
    "    split each sample into substrings (usually words)\n",
    "    recombine substrings into tokens (usually ngrams)\n",
    "    index tokens (associate a unique int value with each token)\n",
    "    transform each sample using this index, either into a vector of ints or a dense float vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84063bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE=5000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE, standardize=\"lower_and_strip_punctuation\", pad_to_max_tokens= True)\n",
    "encoder.adapt(train_ds.map(lambda text, label: text), batch_size= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3d1c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e84aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b329cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59adb335",
   "metadata": {},
   "source": [
    "View Examples of Encoded Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d079be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example = encoder('encanto we dont talk about bruno no no').numpy()\n",
    "encoded_example[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01ea6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoder.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be5f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder(data['Text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa821885",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example, label in train_ds.take(1):\n",
    "  print('texts: ', example.numpy()[:1])\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6280470",
   "metadata": {},
   "source": [
    "### 2.2) Genre Classification with LSTM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7000ccf2",
   "metadata": {},
   "source": [
    "Let's first create LSTM models that are capable of classifying movie reviews into one of four genres: 1) horror, 2) science fiction, 3) comedy, or 4) action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aba6ec4",
   "metadata": {},
   "source": [
    "#### 2.2.1) Genre Classification LSTM Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ac360b",
   "metadata": {},
   "source": [
    "Build the LSTM Model. For more information about layers in keras, refer to this resource: https://www.tensorflow.org/api_docs/python/tf/keras/layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c11debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k.clear_session()\n",
    "num_classes=4\n",
    "model=tf.keras.Sequential([encoder\n",
    "   ,tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True)\n",
    "   ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True, dropout=0.3))\n",
    "   ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,dropout=0.3))\n",
    "   ,tf.keras.layers.Dense(64, activation='relu')\n",
    "   ,tf.keras.layers.Dense(num_classes,activation='softmax')  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c19b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= tf.keras.optimizers.Adam( )\n",
    "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() \n",
    "              ,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63732425",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(train_ds\n",
    "         ,epochs=200\n",
    "         ,validation_data=val_ds\n",
    "         ,validation_steps=3\n",
    "         ,callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d475bb",
   "metadata": {},
   "source": [
    "View a Summary of the LSTM Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a1a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08615d5",
   "metadata": {},
   "source": [
    "Assess Model's Performance Using Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc62041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c550893",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc73a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37de841",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c7c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fd5906",
   "metadata": {},
   "source": [
    "Assess the Model's Performance Using Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c7651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2 = model.predict(test_ds)\n",
    "y_pred2 = np.argmax(preds2, axis=1)\n",
    "y2 = np.concatenate([y for x, y in test_ds], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a929ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES_LIST = ['Action','Comedy','Horror','Sci Fi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4d2aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix_labeled(y2,y_pred2, CLASSES_LIST=CLASSES_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa93ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sns.light_palette((260, 75, 60), input=\"husl\", as_cmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d48af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(preds2[0:15]\n",
    "                  ,columns = CLASSES_LIST).T\n",
    "df2.style.format(\"{:.2%}\").background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5098a87",
   "metadata": {},
   "source": [
    "#### 2.2.2) Genre Classification LSTM Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe66f02b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a5d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881d10fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31594162",
   "metadata": {},
   "source": [
    "#### 2.2.3) Genre Classification LSTM Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa8cd64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6d11e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaadfe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fa3bc3a",
   "metadata": {},
   "source": [
    "### 2.3) Sentiment Analysis and Classification with LSTM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224b59b4",
   "metadata": {},
   "source": [
    "Let's now turn our attention toward creating LSTM models capable of classifying movie reviews as having either positive or negative sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90fbba3",
   "metadata": {},
   "source": [
    "#### 2.3.1) Sentiment Classification LSTM Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390174a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e720338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958cde2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "554d4d02",
   "metadata": {},
   "source": [
    "#### 2.3.2) Sentiment Classification LSTM Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c85328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9aa36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa57188b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d250fa56",
   "metadata": {},
   "source": [
    "#### 2.3.3) Sentiment Classification LSTM Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d838e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39867ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e762cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
