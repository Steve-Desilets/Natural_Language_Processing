{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4769493e",
   "metadata": {},
   "source": [
    "# Assignment 4 - Chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad11a284",
   "metadata": {},
   "source": [
    "#### Steve Desilets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a07da28",
   "metadata": {},
   "source": [
    "#### August 26, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2258b831",
   "metadata": {},
   "source": [
    "## 1) Chatbot 1 - Sentence-Based Transformer Chatbot That Leverages Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d07df60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\steve\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\steve\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.64.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\steve\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\steve\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\steve\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\steve\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.7.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\steve\\anaconda3\\lib\\site-packages (from sentence-transformers) (3.7)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\steve\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.16.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\steve\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\steve\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2022.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\steve\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\steve\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\steve\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (2.11.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\steve\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.3.15)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.2)\n",
      "Requirement already satisfied: click in c:\\users\\steve\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\steve\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers) (9.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\steve\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\steve\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from timeit import default_timer as timer\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "!pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from typing import List, Callable, Dict, Tuple, Set\n",
    "\n",
    "pd.set_option('max_colwidth', 600)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90193bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Sentence Transformer model optimized for  sentence cosine similarity calculations\n",
    "\n",
    "#The models below fully downloaded in Google Colab. This is the version of the google colab notebook but \n",
    "#it was open in anoconda to be saved as pdf and the download graphics did not transfer properly so \n",
    "#it seems like it didn't download. However, it did in the orignal google colab notebook, where all the\n",
    "#analysis was run.\n",
    "\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c7d2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "825d584d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only run this once, they will be downloaded.\n",
    "nltk.download('stopwords',quiet=True)\n",
    "nltk.download('wordnet',quiet=True)\n",
    "nltk.download('punkt',quiet=True)\n",
    "nltk.download('omw-1.4',quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09f9e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "CORPUS_PATH = 'C:/Users/steve/OneDrive/Desktop/Github/Natural_Language_Processing/Chatbot Assignment/coral_reef_fish.txt'\n",
    "f=open(CORPUS_PATH,'r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "raw=raw.lower()# converts to lowercase\n",
    "\n",
    "#create list of sentences and words\n",
    "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw)# converts to list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c670258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create greetings and greetings function\n",
    "\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"Hello\"]\n",
    "\n",
    "\n",
    "# Checking for greetings\n",
    "def greeting(sentence):\n",
    "    \"\"\"If user's input is a greeting, return a greeting response\"\"\"\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c53737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generating response function \n",
    "def response(user_response):\n",
    "    chatbot_response=''\n",
    "    sentence_encodings=model.encode(sent_tokens, convert_to_tensor=True)# generate sentence transformer embeddings\n",
    "    sentence_encodings=sentence_encodings.cpu()\n",
    "    vals = cosine_similarity(sentence_encodings[-1].reshape(1, -1), sentence_encodings) #the chatbot conversation code \n",
    "    #in the next cell adds the question as the last sentence of the sentence tokens, before calling this response function.\n",
    "    #The code takes the last sentence (which is the question) and gets cosine similarities vs all the sentences in the corpus,\n",
    "    #including itself\n",
    "    idx=vals.argsort()[0][-2] #gets the index of the second highest similarity (the first highest would be the question itself)\n",
    "    flat = vals.flatten()#reduces dimension of cosine similarity array to be able to sort\n",
    "    flat.sort() #sort the cosine similarity values\n",
    "    second_cos_sim_val = flat[-2] #get the second highest cosine similarity value.\n",
    "    if(second_cos_sim_val==0): #check the second highest cosine similarity value. If it's zero return the no match response,\n",
    "        #else return highest cosine similarity sentence.\n",
    "        chatbot_response=chatbot_response+\"Sorry, I do not have an answer to your question in my database\"\n",
    "        return chatbot_response\n",
    "    else:\n",
    "        chatbot_response = chatbot_response+sent_tokens[idx] #use index of highest cosine similarity to get original sentence\n",
    "        return chatbot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0489805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Informational Chatbot About Coral Reef Fish. To end session please type exit.\n",
      "\n",
      "\n",
      "How many species of fish live in coral reefs?\n",
      "Answer: the fish that inhabit coral reefs are numerous and diverse.\n",
      "\n",
      "\n",
      "What is the most venomous fish?\n",
      "Answer: the most venomous known fish is the reef stonefish.\n",
      "\n",
      "\n",
      "What sharks live in coral reefs?\n",
      "Answer: the whitetip reef shark almost exclusively inhabits coral reefs.\n",
      "\n",
      "\n",
      "What fish are poisonous?\n",
      "Answer: there is a distinction between poisonous fish and venomous fish.\n",
      "\n",
      "\n",
      "What fish can electrocute you?\n",
      "Answer: some unmistakable contrasting patterns are used to warn predators that the fish has venomous spines or poisonous flesh.\n",
      "\n",
      "\n",
      "What species can sting you?\n",
      "Answer: the most venomous known fish is the reef stonefish.\n",
      "\n",
      "\n",
      "What species are parasitic?\n",
      "Answer: monogenean parasites of the genus pseudorhabdosynochus (arrows) on the gill filament of a grouper.\n",
      "\n",
      "\n",
      "What species are venomous?\n",
      "Answer: few of these venoms have been studied.\n",
      "\n",
      "\n",
      "What species are known for attacking scuba divers?\n",
      "Answer: they are swift predators who patrol the reef in hunting packs.\n",
      "\n",
      "\n",
      "What are common herbivorous fish?\n",
      "Answer: herbivores feed on plants.\n",
      "\n",
      "\n",
      "Why do fish camouflage themselves?\n",
      "Answer: sometimes they camouflage the fish when the fish rests in places with the right background.\n",
      "\n",
      "\n",
      "How do fish get rid of their parasites?\n",
      "Answer: some fish specialise as cleaner fish, and establish cleaning stations where other fish can come to have their parasites nibbled away.\n",
      "\n",
      "\n",
      "What species have mutualistic relationships?\n",
      "Answer: the relationship can be mutualistic, when both species benefit from the relationship, commensalistic, when one species benefits and the other is unaffected, and parasitistic, when one species benefits, and the other is harmed.\n",
      "\n",
      "\n",
      "What species have commensalistic relationships?\n",
      "Answer: the relationship can be mutualistic, when both species benefit from the relationship, commensalistic, when one species benefits and the other is unaffected, and parasitistic, when one species benefits, and the other is harmed.\n",
      "\n",
      "\n",
      "Where in the world are coral reefs found?\n",
      "Answer: they are frequently found near the drop-offs at the outer edges of the reef, and less commonly within lagoons.\n",
      "\n",
      "\n",
      "What species engage in schooling?\n",
      "Answer: schooling fish have developed remarkable displays of precise choreography which confuse and evade predators.\n",
      "\n",
      "\n",
      "Which species are ambush predators?\n",
      "Answer: another ambush predator is the striated frogfish (right).\n",
      "\n",
      "\n",
      "What species can inflate themselves?\n",
      "Answer: they inflate their body by swallowing water, reducing potential predators to those with much bigger mouths.\n",
      "\n",
      "\n",
      "Why are some coral reef fish colorful?\n",
      "Answer: coral reef fishes exhibit a huge variety of dazzling and sometimes bizarre colours and patterns.\n",
      "\n",
      "\n",
      "What species is known to eat birds?\n",
      "Answer: they are active predators of small bony fishes, cephalopods, and crustaceans, and also feed on sea snakes and seabirds.\n",
      "\n",
      "\n",
      "exit\n",
      "Thank you for using the Informational Chatbot About Coral Reef Fish. Goodbye.\n"
     ]
    }
   ],
   "source": [
    "#Chatbot interaction code\n",
    "\n",
    "flag=True\n",
    "print(\"Welcome to the Informational Chatbot About Coral Reef Fish. To end session please type exit.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if user_response!='exit':\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"Answer: You are welcome!\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"Answer: \"+greeting(user_response))\n",
    "            else:\n",
    "                sent_tokens.append(user_response)\n",
    "                word_tokens=word_tokens+nltk.word_tokenize(user_response)\n",
    "                final_words=list(set(word_tokens))\n",
    "                print(\"Answer: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                print(\"\\n\")\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"Thank you for using the Informational Chatbot About Coral Reef Fish. Goodbye.\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3fefc18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_models = []\n",
    "\n",
    "for i in range(20):\n",
    "    chatbot_models.append(1)\n",
    "    \n",
    "chatbot_performance = [\"Incorrect\",\n",
    "                          \"Correct\",\n",
    "                          \"Correct\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Correct\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Partially Correct\",\n",
    "                          \"Correct\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Correct\",\n",
    "                          \"Incorrect\"]\n",
    "\n",
    "######################### NEED TO ADD LAST 2 QUESTIONS ##################################3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f841ea22",
   "metadata": {},
   "source": [
    "## 2) Chatbot 2 - Fine-Tune GPT2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbe9f030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\steve\\anaconda3\\lib\\site-packages (4.31.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\steve\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\steve\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\steve\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in c:\\users\\steve\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\steve\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2022.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\steve\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5a3a1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# Define multiple question and answer pairs\n",
    "qa_pairs = [\n",
    "    (\"Which species are at the top of the food chain?\", \"sharks and giant moray. STOP\"),\n",
    "    (\"What fish eat coral?\", \"parrotfish and butterflyfish. STOP\"),\n",
    "    (\"Which coral reef fish has the shortest lifespan?\", \"seven-figure pygmy goby. STOP\"),\n",
    "    (\"What species can inflate themselves?\", \"puffers, striated frogfish, and porcupinefish. STOP\"),\n",
    "    (\"How do sea anemones protect themselves?\", \"the tentacles of sea anemones bristle with tiny harpoons (nematocysts) primed with toxins, and are an effective deterrent against most predators. STOP\"),\n",
    "    (\"What species commonly serves as a cleaner fish?\", \"bluestreak cleaner wrasse. STOP\"),\n",
    "    (\"What species eat sponges?\", \"emperor angelfish. STOP\"),\n",
    "    (\"What species eat stingrays?\", \"caribbean reef shark and great hammerheads. STOP\"),\n",
    "    (\"Which species are hermaphrodites\", \"grouper. STOP\"),\n",
    "    (\"What species is known to eat birds?\", \"blacktip reef shark. STOP\")\n",
    "]\n",
    "\n",
    "# Concatenate the question and answer pairs with appropriate formatting\n",
    "formatted_pairs = [f\"Q: {q}\\nA: {a}\\n\" for q, a in qa_pairs]\n",
    "qa_text = \"\\n\".join(formatted_pairs)\n",
    "\n",
    "# Fine-tune the GPT-2 model with the Q&A pairs\n",
    "inputs = tokenizer.encode(qa_text, return_tensors=\"pt\")\n",
    "model.train()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Run the fine-tuning loop (example: 1 epoch)\n",
    "for j in range(120):\n",
    "    print (j)\n",
    "    outputs = model(inputs, labels=inputs)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"fine-tuned-gpt2\")\n",
    "\n",
    "# Load the fine-tuned model\n",
    "fine_tuned_model = GPT2LMHeadModel.from_pretrained(\"fine-tuned-gpt2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e70c7504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completion for the test question:\n",
      "['Q: How many species of fish live in coral reefs?', 'A: About 1,000 species of coral reef fish live in the United States. These fish are an important food source for many species of birds and other marine life. However, coral reef fish are an invasive species and are an invasive species throughout the United States. What species are at the top of the food chain?']\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "test_question = \"Q: How many species of fish live in coral reefs?\"\n",
    "\n",
    "# Generate a completion for the test question\n",
    "input_ids = tokenizer.encode(test_question, return_tensors=\"pt\")\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100)\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated completion for the test question:\")\n",
    "print(completion.split(\"STOP\")[0].splitlines()[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "813750e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completion for the test question:\n",
      "['Q: What is the most venomous fish?', 'A: parrotfish and butterflyfish. Parrotfish are the most venomous fish in the world, and are the most venomous fish in the world. Butterflyfish are the most venomous fish in the world, and are the most venomous fish in the world.']\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "test_question = \"Q: What is the most venomous fish?\"\n",
    "\n",
    "# Generate a completion for the test question\n",
    "input_ids = tokenizer.encode(test_question, return_tensors=\"pt\")\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100)\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated completion for the test question:\")\n",
    "print(completion.split(\"STOP\")[0].splitlines()[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec8755ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completion for the test question:\n",
      "['Q: What sharks live in coral reefs?', 'A: sharks and giant moray. ']\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "test_question = \"Q: What sharks live in coral reefs?\"\n",
    "\n",
    "# Generate a completion for the test question\n",
    "input_ids = tokenizer.encode(test_question, return_tensors=\"pt\")\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100)\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated completion for the test question:\")\n",
    "print(completion.split(\"STOP\")[0].splitlines()[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c8cb9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completion for the test question:\n",
      "['Q: What fish are poisonous?', 'A: sharks and giant moray. ']\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "test_question = \"Q: What fish are poisonous?\"\n",
    "\n",
    "# Generate a completion for the test question\n",
    "input_ids = tokenizer.encode(test_question, return_tensors=\"pt\")\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100)\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated completion for the test question:\")\n",
    "print(completion.split(\"STOP\")[0].splitlines()[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e47753b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completion for the test question:\n",
      "['Q: What fish can electrocute you?', 'A: sharks and giant moray. ']\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "test_question = \"Q: What fish can electrocute you?\"\n",
    "\n",
    "# Generate a completion for the test question\n",
    "input_ids = tokenizer.encode(test_question, return_tensors=\"pt\")\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100)\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated completion for the test question:\")\n",
    "print(completion.split(\"STOP\")[0].splitlines()[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c867a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completion for the test question:\n",
      "['Q: What species can sting you?', 'A: sharks and giant moray. ']\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "test_question = \"Q: What species can sting you?\"\n",
    "\n",
    "# Generate a completion for the test question\n",
    "input_ids = tokenizer.encode(test_question, return_tensors=\"pt\")\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100)\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated completion for the test question:\")\n",
    "print(completion.split(\"STOP\")[0].splitlines()[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0c326e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completion for the test question:\n",
      "['Q: What species are parasitic?', 'A: sharks and giant moray. ']\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "test_question = \"Q: What species are parasitic?\"\n",
    "\n",
    "# Generate a completion for the test question\n",
    "input_ids = tokenizer.encode(test_question, return_tensors=\"pt\")\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100)\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated completion for the test question:\")\n",
    "print(completion.split(\"STOP\")[0].splitlines()[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc469a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completion for the test question:\n",
      "['Q: What species are venomous?', 'A: sharks and giant moray. ']\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "test_question = \"Q: What species are venomous?\"\n",
    "\n",
    "# Generate a completion for the test question\n",
    "input_ids = tokenizer.encode(test_question, return_tensors=\"pt\")\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100)\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated completion for the test question:\")\n",
    "print(completion.split(\"STOP\")[0].splitlines()[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5747882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completion for the test question:\n",
      "['Q: What species are known for attacking scuba divers?', 'A: sharks and giant moray. ']\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "test_question = \"Q: What species are known for attacking scuba divers?\"\n",
    "\n",
    "# Generate a completion for the test question\n",
    "input_ids = tokenizer.encode(test_question, return_tensors=\"pt\")\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100)\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated completion for the test question:\")\n",
    "print(completion.split(\"STOP\")[0].splitlines()[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b5c0d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completion for the test question:\n",
      "['Q: What are common herbivorous fish?', 'A: puffers, striated frogfish, and porcupinefish. They are great at eating invertebrates, and are an excellent source of vitamin A. They are an excellent source of calcium, and are an excellent source of iron.']\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "test_question = \"Q: What are common herbivorous fish?\"\n",
    "\n",
    "# Generate a completion for the test question\n",
    "input_ids = tokenizer.encode(test_question, return_tensors=\"pt\")\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100)\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated completion for the test question:\")\n",
    "print(completion.split(\"STOP\")[0].splitlines()[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5598e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completion for the test question:\n",
      "['Q: Why do fish camouflage themselves?', 'A: Because fish camouflage themselves. They are the most effective predators on land. They are the most effective predators on fish. They are the most effective predators on birds. They are the most effective predators on fish. They are the most effective predators on fish. They are the most effective predators on fish. They are the most effective predators on fish. They are the most effective predators on fish. They are the most effective predators on fish. They are the']\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "test_question = \"Q: Why do fish camouflage themselves?\"\n",
    "\n",
    "# Generate a completion for the test question\n",
    "input_ids = tokenizer.encode(test_question, return_tensors=\"pt\")\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100)\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated completion for the test question:\")\n",
    "print(completion.split(\"STOP\")[0].splitlines()[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4137113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completion for the test question:\n",
      "['Q: How do fish get rid of their parasites?', 'A: Fish eat parasites, and fish eat fish. Fish eat parasites because fish eat parasites. Fish eat fish because fish eat parasites. Fish eat fish because fish eat parasites. Fish eat fish because fish eat parasites. Fish eat fish because fish eat parasites. Fish eat fish because fish eat parasites. Fish eat fish because fish eat parasites. Fish eat fish because fish eat parasites. Fish eat fish because fish eat parasites. Fish eat fish because fish']\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "test_question = \"Q: How do fish get rid of their parasites?\"\n",
    "\n",
    "# Generate a completion for the test question\n",
    "input_ids = tokenizer.encode(test_question, return_tensors=\"pt\")\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100)\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated completion for the test question:\")\n",
    "print(completion.split(\"STOP\")[0].splitlines()[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61af525e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completion for the test question:\n",
      "['Q: What species have a mutualistic relationship?', 'A: sharks and giant moray. ']\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "test_question = \"Q: What species have a mutualistic relationship?\"\n",
    "\n",
    "# Generate a completion for the test question\n",
    "input_ids = tokenizer.encode(test_question, return_tensors=\"pt\")\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100)\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated completion for the test question:\")\n",
    "print(completion.split(\"STOP\")[0].splitlines()[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c4aa97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completion for the test question:\n",
      "['Q: What species have a commensalistic relationship?', 'A: sharks and giant moray. ']\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "test_question = \"Q: What species have a commensalistic relationship?\"\n",
    "\n",
    "# Generate a completion for the test question\n",
    "input_ids = tokenizer.encode(test_question, return_tensors=\"pt\")\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100)\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated completion for the test question:\")\n",
    "print(completion.split(\"STOP\")[0].splitlines()[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0881250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completion for the test question:\n",
      "['Q: Where in the world are coral reefs found?', 'A: Coral reefs are the largest coral reef in the world. They are the largest coral reef in the world because of their size and because of their ability to withstand the elements. They are the most abundant coral reef fish in the world. They are an important food source for birds, and are an important food source for sea anemones. They are an excellent predator for fish and are an excellent finisher for reef fish.']\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "test_question = \"Q: Where in the world are coral reefs found?\"\n",
    "\n",
    "# Generate a completion for the test question\n",
    "input_ids = tokenizer.encode(test_question, return_tensors=\"pt\")\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100)\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated completion for the test question:\")\n",
    "print(completion.split(\"STOP\")[0].splitlines()[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba368b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completion for the test question:\n",
      "['Q: What species engage in schooling?', 'A: sharks and giant moray. ']\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "test_question = \"Q: What species engage in schooling?\"\n",
    "\n",
    "# Generate a completion for the test question\n",
    "input_ids = tokenizer.encode(test_question, return_tensors=\"pt\")\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100)\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated completion for the test question:\")\n",
    "print(completion.split(\"STOP\")[0].splitlines()[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49192947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completion for the test question:\n",
      "['Q: What species are ambush predators?', 'A: sharks and giant moray. ']\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "test_question = \"Q: What species are ambush predators?\"\n",
    "\n",
    "# Generate a completion for the test question\n",
    "input_ids = tokenizer.encode(test_question, return_tensors=\"pt\")\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100)\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated completion for the test question:\")\n",
    "print(completion.split(\"STOP\")[0].splitlines()[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a70f8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completion for the test question:\n",
      "['Q: Which species are ambush predators?', 'A: sharks and giant moray. ']\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "test_question = \"Q: Which species are ambush predators?\"\n",
    "\n",
    "# Generate a completion for the test question\n",
    "input_ids = tokenizer.encode(test_question, return_tensors=\"pt\")\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100)\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated completion for the test question:\")\n",
    "print(completion.split(\"STOP\")[0].splitlines()[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6251dfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completion for the test question:\n",
      "['Q: Why are some coral reef fish colorful?', 'A: Coral reef fish are known to eat fish, and are an effective deterrent against most predators. However, coral reef fish are also known to eat stingrays, which are an effective deterrent against most predators.']\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "test_question = \"Q: Why are some coral reef fish colorful?\"\n",
    "\n",
    "# Generate a completion for the test question\n",
    "input_ids = tokenizer.encode(test_question, return_tensors=\"pt\")\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100)\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated completion for the test question:\")\n",
    "print(completion.split(\"STOP\")[0].splitlines()[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6ad9d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completion for the test question:\n",
      "['Q: Which species is blue?', 'A: sharks and giant moray. ']\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "test_question = \"Q: Which species is blue?\"\n",
    "\n",
    "# Generate a completion for the test question\n",
    "input_ids = tokenizer.encode(test_question, return_tensors=\"pt\")\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100)\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated completion for the test question:\")\n",
    "print(completion.split(\"STOP\")[0].splitlines()[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca257f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated completion for the test question:\n",
      "['Q: What is the slowest species that lives in coral reefs?', 'A: sharks and giant moray. ']\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a question\n",
    "test_question = \"Q: What is the slowest species that lives in coral reefs?\"\n",
    "\n",
    "# Generate a completion for the test question\n",
    "input_ids = tokenizer.encode(test_question, return_tensors=\"pt\")\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100)\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated completion for the test question:\")\n",
    "print(completion.split(\"STOP\")[0].splitlines()[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5db50231",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    chatbot_models.append(2)\n",
    "    \n",
    "    \n",
    "model_2_performance_list = [\"Partially Correct\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Partially Correct\",\n",
    "                          \"Incorrect\", \n",
    "                          \"Incorrect\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Partially Correct\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Partially Correct\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Incorrect\"]\n",
    "\n",
    "for i in model_2_performance_list:\n",
    "    chatbot_performance.append(i)\n",
    "    \n",
    "\n",
    "######################### NEED TO ADD LAST 2 QUESTIONS ##################################3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0897e8",
   "metadata": {},
   "source": [
    "## 3) Chatbot 3 - Chatbot Emphasizing Cosine Similarity of TF-IDF Representations of Sententces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91d37018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3fcbde71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('popular', quiet=True) # for downloading packages\n",
    "#nltk.download('punkt') # first-time use only\n",
    "#nltk.download('wordnet') # first-time use only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79d7c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_PATH = 'C:/Users/steve/OneDrive/Desktop/Github/Natural_Language_Processing/Chatbot Assignment/coral_reef_fish.txt'\n",
    "f=open(CORPUS_PATH,'r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "raw = raw.lower()# converts to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cbb3b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw)# converts to list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ff9a1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40864b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"Hi\", \"Hey\", \"*nods*\", \"Hi there\", \"Hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71b91f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eca8b051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am the Third Coral Reef Chatbot! I will answer your queries related to coral reef fish. If you want to exit, type Bye!\n",
      "How many species of fish live in coral reefs?\n",
      " coral reef fish are fish which live amongst or in close relation to coral reefs.\n",
      "What is the most venomous fish?\n",
      " there is a distinction between poisonous fish and venomous fish.\n",
      "What sharks live in coral reefs?\n",
      " coral reef fish are fish which live amongst or in close relation to coral reefs.\n",
      "What fish are poisonous?\n",
      " there is a distinction between poisonous fish and venomous fish.\n",
      "What fish can electrocute you?\n",
      " as with all fish, coral reef fish harbour parasites.\n",
      "What species can sting you?\n",
      " in return, the anemones provide the clownfish protection from their predators, who are not immune to anemone stings.\n",
      "What species are venomous?\n",
      " the most venomous known fish is the reef stonefish.\n",
      "What species are parasitic?\n",
      " parasites of coral reef fish include nematodes, platyhelminthes (cestodes, digeneans, and monogeneans), leeches, parasitic crustaceans such as isopods and copepods, and various microorganisms such as myxosporidia and microsporidia.\n",
      "What species are known for attacking scuba divers?\n",
      " lionfish can aggressively dart at scuba divers and attempt to puncture the facemask with their venomous spines.\n",
      "What are common herbivorous fish?\n",
      " sea anemones are common on reefs.\n",
      "Why do fish camouflage themselves?\n",
      " sometimes they camouflage the fish when the fish rests in places with the right background.\n",
      "How do fish get rid of their parasites?\n",
      " as with all fish, coral reef fish harbour parasites.\n",
      "What species have mutualistic relationships?\n",
      " there is a mutualistic relationship between sea anemones and clownfish.\n",
      "What species have commensalistic relationships?\n",
      " the relationship can be mutualistic, when both species benefit from the relationship, commensalistic, when one species benefits and the other is unaffected, and parasitistic, when one species benefits, and the other is harmed.\n",
      "Where in the world are coral reefs found?\n",
      " coral reefs contain the most diverse fish assemblages to be found anywhere on earth, with perhaps as many as 6,0008,000 species dwelling within coral reef ecosystems of the world's oceans.\n",
      "What species engage in schooling?\n",
      " they have evolved to find protection by schooling, sometimes with other species like shoaling rabbitfish.\n",
      "What species are ambush predators?\n",
      " the well camouflaged striated frogfish, a species of anglerfish, is an ambush predator.\n",
      "Why are some coral reef fish colorful?\n",
      " coral reef fish are fish which live amongst or in close relation to coral reefs.\n",
      "What species is blue?\n",
      " the psychedelic synchiropus splendidus is one of only two animal species known to have blue colouring because of cellular pigment.\n",
      "What is the slowest species that lives in coral reefs?\n",
      " some of these fish parasites have heteroxenous life cycles (i.e.\n",
      "exit\n",
      " after dusk, a group of sharks may target the same prey item, covering every exit route from a particular coral head.\n",
      "bye\n",
      "Bye! take care..\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"Hello, I am the Third Coral Reef Chatbot! I will answer your queries related to coral reef fish. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"You are welcome.\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(greeting(user_response))\n",
    "            else:\n",
    "                print(\"\",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"Bye! take care..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76381b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(20):\n",
    "    chatbot_models.append(3)\n",
    "\n",
    "model_3_performance_list = [\"Incorrect\",\n",
    "                          \"Incorrect\", \n",
    "                          \"Incorrect\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Correct\",\n",
    "                          \"Correct\",\n",
    "                          \"Correct\",\n",
    "                          \"Correct\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Partially Correct\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Correct\",\n",
    "                          \"Incorrect\",\n",
    "                          \"Partially Correct\",\n",
    "                          \"Correct\",\n",
    "                          \"Correct\",\n",
    "                          \"Incorrect\"]\n",
    "\n",
    "for i in model_3_performance_list:\n",
    "    chatbot_performance.append(i)\n",
    "\n",
    "######################### NEED TO ADD LAST 2 QUESTIONS ##################################3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a65da4c",
   "metadata": {},
   "source": [
    "## 4) Chatbot 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71390916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510760b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d6920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e1fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54a5f508",
   "metadata": {},
   "source": [
    "## 5) Evaluation of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56a10569",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_performance_df = pd.DataFrame(list(zip(chatbot_models, chatbot_performance)), \n",
    "                                      columns=['Chatbot Model Number', 'Performance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f22e37b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "no numeric data to plot",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseaborn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mchatbot_performance_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mChatbot Model Number\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstacked\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStacked Barplot of Summarizing Chatbot Performance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mylabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPercent of Responses\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m ax\u001b[38;5;241m.\u001b[39mlegend\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\plotting\\_core.py:972\u001b[0m, in \u001b[0;36mPlotAccessor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    969\u001b[0m             label_name \u001b[38;5;241m=\u001b[39m label_kw \u001b[38;5;129;01mor\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m    970\u001b[0m             data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m label_name\n\u001b[1;32m--> 972\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m plot_backend\u001b[38;5;241m.\u001b[39mplot(data, kind\u001b[38;5;241m=\u001b[39mkind, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\__init__.py:71\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(data, kind, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124max\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(ax, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft_ax\u001b[39m\u001b[38;5;124m\"\u001b[39m, ax)\n\u001b[0;32m     70\u001b[0m plot_obj \u001b[38;5;241m=\u001b[39m PLOT_CLASSES[kind](data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 71\u001b[0m \u001b[43mplot_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m plot_obj\u001b[38;5;241m.\u001b[39mdraw()\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m plot_obj\u001b[38;5;241m.\u001b[39mresult\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:327\u001b[0m, in \u001b[0;36mMPLPlot.generate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args_adjust()\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_plot_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_subplots()\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_plot()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:506\u001b[0m, in \u001b[0;36mMPLPlot._compute_plot_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# no non-numeric frames or series allowed\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_empty:\n\u001b[1;32m--> 506\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno numeric data to plot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m numeric_data\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_to_ndarray)\n",
      "\u001b[1;31mTypeError\u001b[0m: no numeric data to plot"
     ]
    }
   ],
   "source": [
    "plt.style.use('seaborn')\n",
    "chatbot_performance_df.plot(x = 'Chatbot Model Number', \n",
    "                kind = 'bar', \n",
    "                stacked = True,\n",
    "                title = 'Stacked Barplot of Summarizing Chatbot Performance',\n",
    "                ylabel = 'Percent of Responses')\n",
    "\n",
    "ax.legend\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a052d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
